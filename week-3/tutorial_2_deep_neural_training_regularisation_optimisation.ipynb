{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial_2_deep_neural_training_regularisation_optimisation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "DOLXOWVE11KA"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lndrreUlTBLg"
      },
      "source": [
        "# Thinking with Deep Learning: Week 3 Part 2\n",
        "# Optimization and Regularisation for Deep Neural Architectures\n",
        "\n",
        "__Instructor:__ James Evans\n",
        "\n",
        "__Teaching Assistants & Content Creators:__ Bhargav Srinivasa Desikan, Likun Cao\n",
        "\n",
        "In this notebook we will focus on the many techniques which actually get neural networks to run - optimization and regularisation. In the last notebook we focused on quickly getting models whipped up and ready to go for a variety of tasks, data and settings. In this notebook our approach will be quite different, and we will rather focus on working on just one kind of problem, of image classification - by keeping the problem fixed, we can then really focus on what each of the different optimisation and regularisation procedures do.\n",
        "\n",
        "\n",
        "\n",
        "This notebooks features some code and concepts from the UPenn CS-522 course, specifically their [week 4 on Optimisation](https://github.com/CIS-522/course-content/tree/main/tutorials/W04_Optimization), and [week 5 on Regularisation](https://github.com/CIS-522/course-content/tree/main/tutorials/W05_Regularization), and we are grateful for their content and support! We would also highly recommend them as resources for more code and content on optimisation, regularisation, and other content on deep learning.\n",
        "\n",
        "**Notes about the tutorial**: Much like the last tutorial, there is a lot more code here. Once again, a lot of that code is just to setup data and some of the neural nets, and we have tried to comment the code as extensively as possible. Like last time, we highly recommend looking at the homework at the bottom of the notebook so that you can manage your time on the notebook better. Getting to know about the possible range of optimization options and regularisation options is a very helpful (and necessary) exercise to get the most out of your deep neural models. Throughout this notebook, you will also encounter some exploratory questions. You need not answer them in the notebook but you are encouraged to search for the answers (or ask us if you can't find them!).\n",
        "\n",
        "Your purpose in this notebook is to be able to adapt your Keras or PyTorch models with the various techniques we discuss below. All the primary code in the notebook is in PyTorch with links to resources or documentation for similar options in Keras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOLXOWVE11KA"
      },
      "source": [
        "\n",
        "\n",
        "# Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDT9Cui213mV"
      },
      "source": [
        "# imports\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import requests\n",
        "import io\n",
        "from urllib.request import urlopen\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83AdYyW_XHpQ"
      },
      "source": [
        "---\n",
        "## Minibatch stochastic gradient descent (SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCCvrYFbMULT"
      },
      "source": [
        "In stochastic gradient descent, we replace the actual gradient vector with a stochastic estimation of the gradient vector. Specifically for a neural network, the stochastic estimation uses the gradient of the loss for a single data point (single instance).\n",
        "\n",
        "Given $f_i=l(x_i, y_i, w)$, the expected value of the $t$-th step of SGD is the same as the $t$-th step of full gradient descent.\n",
        "\n",
        "$$\\mathbb{E}[w_{t+1}]=w_t-\\eta \\mathbb{E}[\\nabla f_i(w_t)]=w_t-\\eta\\nabla f(w_t)$$\n",
        "\n",
        "where $i$ is chosen uniformly at random, thereby $f_i$ is a noisy but unbiased estimator of $f$.\n",
        "\n",
        "$$w_{t+1}=w_t-\\eta\\nabla f_i(w_t)$$\n",
        "\n",
        "We update the weights according to the gradient over $f_i$ (as opposed to the gradient over the total loss $f$).\n",
        "\n",
        "SGD advantages:\n",
        "*   The noise in the SGD update can prevent convergence to a bad (shallow) local minima.\n",
        "*   It is drastically cheaper to compute (as you donâ€™t go over all data points).\n",
        "\n",
        "\n",
        "### Minibatching\n",
        "\n",
        "Often we are able to make better use of our hardware by using mini batches instead of single instances. We compute the loss over a mini-batch -- a set of randomly selected instances instead of calculating it over just one instance. This reduces the noise in the step update.\n",
        "\n",
        "Given the $t$th minibatch $B_t$ consisting of $k$ observations: \n",
        "\n",
        "$$w_{t+1}=w_t-\\eta \\frac{1}{|B_t|}\\sum_{i\\in B}\\nabla f_i(w_t)$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZ_GaUJ7DGd"
      },
      "source": [
        "One of the main constraints of training deep neural networks is the relative limited size of GPU memory. Being able to quickly estimate if your minibatch size can be held in that memory will save you time and out-of-memory errors.\n",
        "\n",
        "What do we need to store at training time? \n",
        "- outputs of intermediate layers (forward pass): \n",
        "- model parameters\n",
        "- error signal at each neuron\n",
        "- the gradient of parameters\n",
        "plus any extra memory needed by optimizer (e.g. for momentum)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6vakYTg1YcU"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8HmOeDH1ZCM"
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        # set optimiser to zero\n",
        "        optimizer.zero_grad()\n",
        "        # pass the data through the model\n",
        "        output = model(data)\n",
        "        # calculate loss\n",
        "        loss = F.nll_loss(output, target)\n",
        "        # backprop\n",
        "        loss.backward()\n",
        "        # move optimiser\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgKU3GKo1bAC"
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLeBA59k88ON"
      },
      "source": [
        "def main(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    acc_list, time_list = [], []\n",
        "    start_time = time.time()\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        time_list.append(time.time()-start_time)\n",
        "        acc = test(model, device, test_loader)\n",
        "        acc_list.append(acc)\n",
        "\n",
        "    return acc_list, time_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdQDkTa8-pxK"
      },
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 32,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 3,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "batch_size = [8, 16, 32, 64, 256, 512, 1024]\n",
        "acc_dict = {}\n",
        "test_acc = []"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7j305cf8_lW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60c87a9d-3831-4826-b9dc-b61700cac175"
      },
      "source": [
        "\n",
        "\n",
        "for i in range(len(batch_size)):\n",
        "    args['batch_size'] = batch_size[i]\n",
        "    acc, timer = main(args)\n",
        "    acc_dict['acc'+str(batch_size[i])] = acc\n",
        "    acc_dict['time'+str(batch_size[i])] = timer\n",
        "    test_acc.append(acc[-1])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.372123\n",
            "Train Epoch: 1 [800/60000 (1%)]\tLoss: 0.239777\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.448986\n",
            "Train Epoch: 1 [2400/60000 (4%)]\tLoss: 0.682191\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.087143\n",
            "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.204913\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.135260\n",
            "Train Epoch: 1 [5600/60000 (9%)]\tLoss: 0.160104\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.233126\n",
            "Train Epoch: 1 [7200/60000 (12%)]\tLoss: 0.740059\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.026902\n",
            "Train Epoch: 1 [8800/60000 (15%)]\tLoss: 0.109436\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.009062\n",
            "Train Epoch: 1 [10400/60000 (17%)]\tLoss: 0.017327\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.344575\n",
            "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.011081\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.785516\n",
            "Train Epoch: 1 [13600/60000 (23%)]\tLoss: 0.096689\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.313701\n",
            "Train Epoch: 1 [15200/60000 (25%)]\tLoss: 0.029086\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.127179\n",
            "Train Epoch: 1 [16800/60000 (28%)]\tLoss: 1.610911\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.035197\n",
            "Train Epoch: 1 [18400/60000 (31%)]\tLoss: 0.295625\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.103032\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.761033\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.857438\n",
            "Train Epoch: 1 [21600/60000 (36%)]\tLoss: 0.028135\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.071074\n",
            "Train Epoch: 1 [23200/60000 (39%)]\tLoss: 0.456148\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.004165\n",
            "Train Epoch: 1 [24800/60000 (41%)]\tLoss: 0.129142\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.004654\n",
            "Train Epoch: 1 [26400/60000 (44%)]\tLoss: 0.050554\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.018805\n",
            "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.016063\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.003705\n",
            "Train Epoch: 1 [29600/60000 (49%)]\tLoss: 0.344256\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.001117\n",
            "Train Epoch: 1 [31200/60000 (52%)]\tLoss: 0.746211\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.019212\n",
            "Train Epoch: 1 [32800/60000 (55%)]\tLoss: 0.011710\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.024892\n",
            "Train Epoch: 1 [34400/60000 (57%)]\tLoss: 0.071267\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.030578\n",
            "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.003893\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.196244\n",
            "Train Epoch: 1 [37600/60000 (63%)]\tLoss: 0.512601\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.011647\n",
            "Train Epoch: 1 [39200/60000 (65%)]\tLoss: 0.006641\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.001112\n",
            "Train Epoch: 1 [40800/60000 (68%)]\tLoss: 0.398958\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.000143\n",
            "Train Epoch: 1 [42400/60000 (71%)]\tLoss: 0.010863\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 1.348366\n",
            "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.114652\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.855307\n",
            "Train Epoch: 1 [45600/60000 (76%)]\tLoss: 0.005297\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.617777\n",
            "Train Epoch: 1 [47200/60000 (79%)]\tLoss: 0.084855\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.000023\n",
            "Train Epoch: 1 [48800/60000 (81%)]\tLoss: 0.575029\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 1.239793\n",
            "Train Epoch: 1 [50400/60000 (84%)]\tLoss: 0.133625\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.028538\n",
            "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.201285\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.006751\n",
            "Train Epoch: 1 [53600/60000 (89%)]\tLoss: 0.000513\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.040247\n",
            "Train Epoch: 1 [55200/60000 (92%)]\tLoss: 0.061683\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.827805\n",
            "Train Epoch: 1 [56800/60000 (95%)]\tLoss: 0.033880\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.000353\n",
            "Train Epoch: 1 [58400/60000 (97%)]\tLoss: 0.853862\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.265086\n",
            "\n",
            "Test set: Average loss: 0.2371, Accuracy: 9416/10000 (94.1600%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.412121\n",
            "Train Epoch: 2 [800/60000 (1%)]\tLoss: 0.188770\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.109072\n",
            "Train Epoch: 2 [2400/60000 (4%)]\tLoss: 0.360795\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.209469\n",
            "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.007120\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.021351\n",
            "Train Epoch: 2 [5600/60000 (9%)]\tLoss: 0.013602\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.000719\n",
            "Train Epoch: 2 [7200/60000 (12%)]\tLoss: 0.247885\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.218366\n",
            "Train Epoch: 2 [8800/60000 (15%)]\tLoss: 0.000705\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.034568\n",
            "Train Epoch: 2 [10400/60000 (17%)]\tLoss: 0.083363\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.000310\n",
            "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.061054\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.004784\n",
            "Train Epoch: 2 [13600/60000 (23%)]\tLoss: 0.034029\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.113694\n",
            "Train Epoch: 2 [15200/60000 (25%)]\tLoss: 0.024402\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.125118\n",
            "Train Epoch: 2 [16800/60000 (28%)]\tLoss: 0.021606\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.015398\n",
            "Train Epoch: 2 [18400/60000 (31%)]\tLoss: 0.004533\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.002662\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.017842\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.864135\n",
            "Train Epoch: 2 [21600/60000 (36%)]\tLoss: 0.520465\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.006995\n",
            "Train Epoch: 2 [23200/60000 (39%)]\tLoss: 0.055424\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.002533\n",
            "Train Epoch: 2 [24800/60000 (41%)]\tLoss: 0.646434\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.089977\n",
            "Train Epoch: 2 [26400/60000 (44%)]\tLoss: 0.219326\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.905045\n",
            "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.000714\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 1.116317\n",
            "Train Epoch: 2 [29600/60000 (49%)]\tLoss: 0.464041\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.214272\n",
            "Train Epoch: 2 [31200/60000 (52%)]\tLoss: 0.019510\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.004278\n",
            "Train Epoch: 2 [32800/60000 (55%)]\tLoss: 0.075144\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.820721\n",
            "Train Epoch: 2 [34400/60000 (57%)]\tLoss: 0.001883\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.026034\n",
            "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.320508\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.322699\n",
            "Train Epoch: 2 [37600/60000 (63%)]\tLoss: 0.083210\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.132706\n",
            "Train Epoch: 2 [39200/60000 (65%)]\tLoss: 0.000261\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.772346\n",
            "Train Epoch: 2 [40800/60000 (68%)]\tLoss: 0.110365\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.120161\n",
            "Train Epoch: 2 [42400/60000 (71%)]\tLoss: 0.383359\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.529193\n",
            "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.005159\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.412684\n",
            "Train Epoch: 2 [45600/60000 (76%)]\tLoss: 0.162719\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.001489\n",
            "Train Epoch: 2 [47200/60000 (79%)]\tLoss: 0.374799\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 1.643933\n",
            "Train Epoch: 2 [48800/60000 (81%)]\tLoss: 0.000524\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.075229\n",
            "Train Epoch: 2 [50400/60000 (84%)]\tLoss: 1.314822\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.250869\n",
            "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.005628\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.101434\n",
            "Train Epoch: 2 [53600/60000 (89%)]\tLoss: 0.008701\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.062435\n",
            "Train Epoch: 2 [55200/60000 (92%)]\tLoss: 0.001683\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.000424\n",
            "Train Epoch: 2 [56800/60000 (95%)]\tLoss: 0.000016\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.285364\n",
            "Train Epoch: 2 [58400/60000 (97%)]\tLoss: 0.012873\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.000618\n",
            "\n",
            "Test set: Average loss: 0.1993, Accuracy: 9521/10000 (95.2100%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.000805\n",
            "Train Epoch: 3 [800/60000 (1%)]\tLoss: 0.381565\n",
            "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.003255\n",
            "Train Epoch: 3 [2400/60000 (4%)]\tLoss: 0.796017\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.226006\n",
            "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.186249\n",
            "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.000320\n",
            "Train Epoch: 3 [5600/60000 (9%)]\tLoss: 0.000897\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.001382\n",
            "Train Epoch: 3 [7200/60000 (12%)]\tLoss: 0.002424\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.000440\n",
            "Train Epoch: 3 [8800/60000 (15%)]\tLoss: 0.544240\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.000569\n",
            "Train Epoch: 3 [10400/60000 (17%)]\tLoss: 0.024220\n",
            "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.010352\n",
            "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.107002\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.003283\n",
            "Train Epoch: 3 [13600/60000 (23%)]\tLoss: 0.432203\n",
            "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.000003\n",
            "Train Epoch: 3 [15200/60000 (25%)]\tLoss: 0.000352\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.139855\n",
            "Train Epoch: 3 [16800/60000 (28%)]\tLoss: 0.005205\n",
            "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.008824\n",
            "Train Epoch: 3 [18400/60000 (31%)]\tLoss: 0.008383\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 1.757033\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.000026\n",
            "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.025495\n",
            "Train Epoch: 3 [21600/60000 (36%)]\tLoss: 0.066183\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000055\n",
            "Train Epoch: 3 [23200/60000 (39%)]\tLoss: 0.040852\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.000092\n",
            "Train Epoch: 3 [24800/60000 (41%)]\tLoss: 0.003842\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.203910\n",
            "Train Epoch: 3 [26400/60000 (44%)]\tLoss: 0.002419\n",
            "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.104912\n",
            "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.000002\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.000038\n",
            "Train Epoch: 3 [29600/60000 (49%)]\tLoss: 0.000007\n",
            "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.001248\n",
            "Train Epoch: 3 [31200/60000 (52%)]\tLoss: 0.000002\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.001483\n",
            "Train Epoch: 3 [32800/60000 (55%)]\tLoss: 0.000723\n",
            "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.000045\n",
            "Train Epoch: 3 [34400/60000 (57%)]\tLoss: 0.006860\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.014328\n",
            "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.004240\n",
            "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.160925\n",
            "Train Epoch: 3 [37600/60000 (63%)]\tLoss: 0.001536\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.139808\n",
            "Train Epoch: 3 [39200/60000 (65%)]\tLoss: 0.006606\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.904266\n",
            "Train Epoch: 3 [40800/60000 (68%)]\tLoss: 0.176528\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.199438\n",
            "Train Epoch: 3 [42400/60000 (71%)]\tLoss: 0.000794\n",
            "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.000006\n",
            "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.048614\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.000818\n",
            "Train Epoch: 3 [45600/60000 (76%)]\tLoss: 0.000667\n",
            "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.670488\n",
            "Train Epoch: 3 [47200/60000 (79%)]\tLoss: 0.990370\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.607865\n",
            "Train Epoch: 3 [48800/60000 (81%)]\tLoss: 0.615306\n",
            "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.302897\n",
            "Train Epoch: 3 [50400/60000 (84%)]\tLoss: 0.000526\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.000375\n",
            "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.026639\n",
            "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.062727\n",
            "Train Epoch: 3 [53600/60000 (89%)]\tLoss: 0.106007\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.000058\n",
            "Train Epoch: 3 [55200/60000 (92%)]\tLoss: 0.001770\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.001982\n",
            "Train Epoch: 3 [56800/60000 (95%)]\tLoss: 0.006495\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.013752\n",
            "Train Epoch: 3 [58400/60000 (97%)]\tLoss: 0.012512\n",
            "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.001770\n",
            "\n",
            "Test set: Average loss: 0.1999, Accuracy: 9523/10000 (95.2300%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.380137\n",
            "Train Epoch: 1 [1600/60000 (3%)]\tLoss: 0.303925\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.211991\n",
            "Train Epoch: 1 [4800/60000 (8%)]\tLoss: 0.186759\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.219290\n",
            "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.046413\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.053508\n",
            "Train Epoch: 1 [11200/60000 (19%)]\tLoss: 0.305580\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.139829\n",
            "Train Epoch: 1 [14400/60000 (24%)]\tLoss: 0.029772\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.069394\n",
            "Train Epoch: 1 [17600/60000 (29%)]\tLoss: 0.100812\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.545296\n",
            "Train Epoch: 1 [20800/60000 (35%)]\tLoss: 0.361358\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.029612\n",
            "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.033597\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.065080\n",
            "Train Epoch: 1 [27200/60000 (45%)]\tLoss: 0.013384\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.004658\n",
            "Train Epoch: 1 [30400/60000 (51%)]\tLoss: 0.097029\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.021808\n",
            "Train Epoch: 1 [33600/60000 (56%)]\tLoss: 0.019769\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.109380\n",
            "Train Epoch: 1 [36800/60000 (61%)]\tLoss: 0.163177\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.051678\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.036628\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.002353\n",
            "Train Epoch: 1 [43200/60000 (72%)]\tLoss: 0.149890\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.068641\n",
            "Train Epoch: 1 [46400/60000 (77%)]\tLoss: 0.087484\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.095484\n",
            "Train Epoch: 1 [49600/60000 (83%)]\tLoss: 0.283875\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.206677\n",
            "Train Epoch: 1 [52800/60000 (88%)]\tLoss: 0.005818\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.208326\n",
            "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.665456\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.005444\n",
            "Train Epoch: 1 [59200/60000 (99%)]\tLoss: 0.023635\n",
            "\n",
            "Test set: Average loss: 0.1253, Accuracy: 9633/10000 (96.3300%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.067806\n",
            "Train Epoch: 2 [1600/60000 (3%)]\tLoss: 0.166174\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.205149\n",
            "Train Epoch: 2 [4800/60000 (8%)]\tLoss: 0.019096\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.002338\n",
            "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.013227\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.138546\n",
            "Train Epoch: 2 [11200/60000 (19%)]\tLoss: 0.007862\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.024041\n",
            "Train Epoch: 2 [14400/60000 (24%)]\tLoss: 0.019273\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.082074\n",
            "Train Epoch: 2 [17600/60000 (29%)]\tLoss: 0.728865\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.399550\n",
            "Train Epoch: 2 [20800/60000 (35%)]\tLoss: 0.168029\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.001919\n",
            "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.645635\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.025103\n",
            "Train Epoch: 2 [27200/60000 (45%)]\tLoss: 0.361661\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.005339\n",
            "Train Epoch: 2 [30400/60000 (51%)]\tLoss: 0.243458\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.044146\n",
            "Train Epoch: 2 [33600/60000 (56%)]\tLoss: 0.341507\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.034686\n",
            "Train Epoch: 2 [36800/60000 (61%)]\tLoss: 0.022199\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.044075\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.578624\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.013994\n",
            "Train Epoch: 2 [43200/60000 (72%)]\tLoss: 0.035196\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.001541\n",
            "Train Epoch: 2 [46400/60000 (77%)]\tLoss: 0.020744\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.026454\n",
            "Train Epoch: 2 [49600/60000 (83%)]\tLoss: 0.047950\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.007255\n",
            "Train Epoch: 2 [52800/60000 (88%)]\tLoss: 0.010987\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.096760\n",
            "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.109082\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.010140\n",
            "Train Epoch: 2 [59200/60000 (99%)]\tLoss: 0.046273\n",
            "\n",
            "Test set: Average loss: 0.1012, Accuracy: 9709/10000 (97.0900%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.080876\n",
            "Train Epoch: 3 [1600/60000 (3%)]\tLoss: 0.002674\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.023250\n",
            "Train Epoch: 3 [4800/60000 (8%)]\tLoss: 0.010786\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.002305\n",
            "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.001332\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.026521\n",
            "Train Epoch: 3 [11200/60000 (19%)]\tLoss: 0.051839\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.000323\n",
            "Train Epoch: 3 [14400/60000 (24%)]\tLoss: 0.000255\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.364083\n",
            "Train Epoch: 3 [17600/60000 (29%)]\tLoss: 0.003636\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.181367\n",
            "Train Epoch: 3 [20800/60000 (35%)]\tLoss: 0.006905\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.000273\n",
            "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.205470\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.032026\n",
            "Train Epoch: 3 [27200/60000 (45%)]\tLoss: 0.223507\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.006089\n",
            "Train Epoch: 3 [30400/60000 (51%)]\tLoss: 0.001981\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.062954\n",
            "Train Epoch: 3 [33600/60000 (56%)]\tLoss: 0.002305\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.025855\n",
            "Train Epoch: 3 [36800/60000 (61%)]\tLoss: 0.046428\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.369729\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.307574\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.512754\n",
            "Train Epoch: 3 [43200/60000 (72%)]\tLoss: 0.013101\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.001920\n",
            "Train Epoch: 3 [46400/60000 (77%)]\tLoss: 0.352447\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.050354\n",
            "Train Epoch: 3 [49600/60000 (83%)]\tLoss: 0.046547\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.003809\n",
            "Train Epoch: 3 [52800/60000 (88%)]\tLoss: 0.016942\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.002212\n",
            "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.002851\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.004662\n",
            "Train Epoch: 3 [59200/60000 (99%)]\tLoss: 0.083694\n",
            "\n",
            "Test set: Average loss: 0.1120, Accuracy: 9698/10000 (96.9800%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.336379\n",
            "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.325145\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.241521\n",
            "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.264007\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.209419\n",
            "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.070037\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.482454\n",
            "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.136715\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.344303\n",
            "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.035363\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162355\n",
            "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.239530\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.025838\n",
            "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.069388\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.111215\n",
            "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.088635\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.177850\n",
            "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.106523\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.035892\n",
            "\n",
            "Test set: Average loss: 0.1223, Accuracy: 9607/10000 (96.0700%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.146243\n",
            "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.053278\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.059891\n",
            "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.053592\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.189994\n",
            "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.052941\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.312451\n",
            "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.021946\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.098019\n",
            "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.012607\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.121977\n",
            "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.096567\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.213233\n",
            "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.014134\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.017219\n",
            "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.190628\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.083750\n",
            "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.040619\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.090055\n",
            "\n",
            "Test set: Average loss: 0.0821, Accuracy: 9742/10000 (97.4200%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.134218\n",
            "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.058866\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.019784\n",
            "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.094042\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.001722\n",
            "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.190808\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.088748\n",
            "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.001595\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041418\n",
            "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.006058\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.095708\n",
            "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.005011\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.102988\n",
            "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.189204\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.053024\n",
            "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.075108\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.231285\n",
            "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.041664\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.103214\n",
            "\n",
            "Test set: Average loss: 0.0814, Accuracy: 9755/10000 (97.5500%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.422726\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.396150\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.378128\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.364550\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.193862\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.145549\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.164392\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.200299\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.138219\n",
            "\n",
            "Test set: Average loss: 0.1403, Accuracy: 9575/10000 (95.7500%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.086524\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.078450\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093668\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.189943\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.258257\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.152435\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.093809\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.084670\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.114840\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.074817\n",
            "\n",
            "Test set: Average loss: 0.0999, Accuracy: 9700/10000 (97.0000%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.091788\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.057657\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.012175\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.130410\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.093741\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.032666\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066307\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.052721\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.134493\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.055678\n",
            "\n",
            "Test set: Average loss: 0.0859, Accuracy: 9732/10000 (97.3200%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312012\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.441432\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.267170\n",
            "\n",
            "Test set: Average loss: 0.2656, Accuracy: 9248/10000 (92.4800%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.268211\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.187153\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.239970\n",
            "\n",
            "Test set: Average loss: 0.1945, Accuracy: 9450/10000 (94.5000%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.225232\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.204634\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.237081\n",
            "\n",
            "Test set: Average loss: 0.1557, Accuracy: 9543/10000 (95.4300%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306117\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.304395\n",
            "\n",
            "Test set: Average loss: 0.3155, Accuracy: 9092/10000 (90.9200%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.340320\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.284743\n",
            "\n",
            "Test set: Average loss: 0.2552, Accuracy: 9277/10000 (92.7700%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.263995\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.281635\n",
            "\n",
            "Test set: Average loss: 0.2199, Accuracy: 9375/10000 (93.7500%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.306466\n",
            "\n",
            "Test set: Average loss: 0.3767, Accuracy: 8912/10000 (89.1200%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.407418\n",
            "\n",
            "Test set: Average loss: 0.3072, Accuracy: 9108/10000 (91.0800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.320780\n",
            "\n",
            "Test set: Average loss: 0.2767, Accuracy: 9204/10000 (92.0400%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhifTHX89CH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "outputId": "26e93f8f-46c5-4df3-db5b-4ee3ed2e7b66"
      },
      "source": [
        "\n",
        "plt.plot(batch_size, test_acc, linewidth=2)\n",
        "plt.title('Optimal Minibach Size')\n",
        "plt.ylabel('Test Accuracy (%)')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Batch Size')\n",
        "plt.savefig('minibatch.png')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uSRuwiTXTFO"
      },
      "source": [
        "---\n",
        "## Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWo8HcvjR_-a"
      },
      "source": [
        "Rather than improving the optimization algorithms, batch normalization improves the network structure itself by adding additional layers in between existing layers. The goal is to improve the optimization and generalization performance.\n",
        "\n",
        "In neural networks, we typically alternate linear (weighted summation) operations with non-linear operations, the activation functions, such as ReLU. The most common practice is to put the normalization is between the linear layers and activation functions.\n",
        "\n",
        "More formally, normalization is as follows:\n",
        "$$\\tilde x_j = a\\frac{x_j-\\mu_j}{\\sigma_j}+b$$\n",
        "where\n",
        "*   $x_j$ is the output of a neuron or, equivalently, the input to the next layer,\n",
        "*   $\\tilde x_j$ is that same feature after being normalized ,\n",
        "*   $\\mu_j$ is the mean of the feature $x_j$ over the minibatch,\n",
        "*   $\\sigma_j$ is the estimate of the standard deviation of $x_j$ over the minibatch (with $\\epsilon$ added, so we don't divide by zero),\n",
        "*   $a$ is the learnable scaling factor,\n",
        "*   $b$ is the learnable bias term.\n",
        "\n",
        "Batch normalization tries to reduce the â€œinternal covariate shiftâ€ between the training and testing data. Internal covariate shift is the change in the distribution of network activations due to the change in paramaters during training. In neural networks, the output of the first layer feeds into the second layer, the output of the second layer feeds into the third, and so on. When the parameters of a layer change, so does the distribution of inputs to subsequent layers. These shifts in input distributions can be problematic for neural networks, especially deep neural networks that could have a large number of layers. Batch normalization tries to mitigate this. You can check out [this](https://arxiv.org/abs/1502.03167) paper where the idea of mitigating internal covariance shift with batch normalization was first introduced. \n",
        "\n",
        "\n",
        "The advantages of BN are as follows:\n",
        "\n",
        "*   Networks with normalization layers are easier to optimize, allowing for the use of larger learning rates, speeding up the training of neural networks.\n",
        "*   The mean/std deviation estimates are noisy due to the randomness of the samples in batch. This extra â€œnoiseâ€ sometimes results in better generalization. Normalization has a regularization effect.\n",
        "*   Normalization reduces sensitivity to weight initialization.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNErTZNI9Eyk"
      },
      "source": [
        "#help functions\n",
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss = 0.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss /= len(train_loader.dataset)\n",
        "    return avg_loss\n",
        "            \n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return test_loss\n",
        "\n",
        "def bn_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    if args['net_type'] == 'Shallow':\n",
        "        model = Net().to(device)\n",
        "    elif args['net_type'] == 'BNShallow':\n",
        "        model = BNShallowNet().to(device)\n",
        "    elif args['net_type'] == 'Deep':\n",
        "        model = DeepNet().to(device)\n",
        "    elif args['net_type'] == 'BNDeep':\n",
        "        model = BNDeepNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train_loss = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        test_loss = test(model, device, test_loader)\n",
        "        train_list.append(train_loss)\n",
        "        test_list.append(test_loss)\n",
        "\n",
        "    return train_list, test_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS82WuPv1j2x"
      },
      "source": [
        "#models\n",
        "class BNShallowNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNShallowNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.bn = nn.BatchNorm1d(128)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnGtouRb1lt1"
      },
      "source": [
        "class BNDeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BNDeepNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 32)\n",
        "        self.fc5 = nn.Linear(32, 10)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.bn2 = nn.BatchNorm1d(64)\n",
        "        self.bn3 = nn.BatchNorm1d(32)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHJAwUgN9MFo"
      },
      "source": [
        "class DeepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 32)\n",
        "        self.fc5 = nn.Linear(32, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc5(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9bFT2Ob9Trh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b9ede65-07cf-4db4-9456-4a489d1b7423"
      },
      "source": [
        "#train\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "net = ['Shallow', 'BNShallow', 'Deep', 'BNDeep']\n",
        "loss_dict = {}\n",
        "\n",
        "for i in range(len(net)):\n",
        "    args['net_type'] = net[i]\n",
        "    train_loss, test_loss = bn_eval(args)\n",
        "    loss_dict['train' + str(net[i])] = train_loss\n",
        "    loss_dict['test' + str(net[i])] = test_loss"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.422726\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.396150\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.378128\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.364550\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.193862\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.145549\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.164392\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.200299\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.138219\n",
            "\n",
            "Test set: Average loss: 0.1403, Accuracy: 9575/10000 (95.7500%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.086524\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.078450\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.093668\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.189943\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.258257\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.152435\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.093809\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.084670\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.114840\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.074817\n",
            "\n",
            "Test set: Average loss: 0.0999, Accuracy: 9700/10000 (97.0000%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.091788\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.057657\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.012175\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.130410\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.093741\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.032666\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.066307\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.052721\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.134493\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.055678\n",
            "\n",
            "Test set: Average loss: 0.0859, Accuracy: 9732/10000 (97.3200%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.021204\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.053360\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.041926\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.032906\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.030425\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.030049\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.030968\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.071048\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.026350\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.014322\n",
            "\n",
            "Test set: Average loss: 0.0807, Accuracy: 9759/10000 (97.5900%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.037836\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.044563\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.029874\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.014201\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.052310\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.096121\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.056224\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.025806\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.082918\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.094313\n",
            "\n",
            "Test set: Average loss: 0.0738, Accuracy: 9774/10000 (97.7400%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.033817\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.010477\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.011654\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.094499\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.047122\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.014987\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.042556\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.016578\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.039261\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.023916\n",
            "\n",
            "Test set: Average loss: 0.0795, Accuracy: 9757/10000 (97.5700%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.013358\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.019543\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.006696\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.015018\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.043979\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.015701\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.013074\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.008557\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.057467\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.040088\n",
            "\n",
            "Test set: Average loss: 0.0827, Accuracy: 9744/10000 (97.4400%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.032270\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.024116\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.024876\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.004699\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.060447\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.017269\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.106864\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.101209\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.043307\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.019857\n",
            "\n",
            "Test set: Average loss: 0.0745, Accuracy: 9767/10000 (97.6700%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.013928\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.012765\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.010794\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.013471\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.005193\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.038694\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.005993\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.027923\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.014283\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.024616\n",
            "\n",
            "Test set: Average loss: 0.0715, Accuracy: 9773/10000 (97.7300%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.044651\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.013484\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.027057\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001918\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.005217\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.006919\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.009823\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.021012\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.027032\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.014536\n",
            "\n",
            "Test set: Average loss: 0.0762, Accuracy: 9767/10000 (97.6700%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.568775\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.418113\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.499692\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.523850\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.523987\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.258274\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.171373\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.328465\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.392123\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.240257\n",
            "\n",
            "Test set: Average loss: 0.1748, Accuracy: 9484/10000 (94.8400%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.234256\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.171225\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.268560\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.345730\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.450819\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.336569\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.176087\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.273994\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.191090\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.185290\n",
            "\n",
            "Test set: Average loss: 0.1400, Accuracy: 9595/10000 (95.9500%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.307299\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.305455\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.053994\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.399332\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.338068\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.091597\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.145726\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.209730\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.291451\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.310419\n",
            "\n",
            "Test set: Average loss: 0.1252, Accuracy: 9629/10000 (96.2900%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.152074\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.243289\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.162329\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.231923\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.102389\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.212083\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.136787\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.104329\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.152024\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.137283\n",
            "\n",
            "Test set: Average loss: 0.1147, Accuracy: 9666/10000 (96.6600%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.198832\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.166579\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.247200\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.121291\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.147823\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.187992\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.218984\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.142880\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.229747\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.206739\n",
            "\n",
            "Test set: Average loss: 0.1039, Accuracy: 9704/10000 (97.0400%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.163419\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.114935\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.251404\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.269778\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.289965\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.051378\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.181026\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.103752\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.088673\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.202593\n",
            "\n",
            "Test set: Average loss: 0.0996, Accuracy: 9707/10000 (97.0700%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.138056\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.142052\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.180594\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.154193\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.151957\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.119326\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.230802\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.126682\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.293555\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.300770\n",
            "\n",
            "Test set: Average loss: 0.0944, Accuracy: 9727/10000 (97.2700%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.268656\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.191191\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.293958\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.117054\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.215660\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.036743\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.220965\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.204490\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.180010\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.165571\n",
            "\n",
            "Test set: Average loss: 0.0917, Accuracy: 9731/10000 (97.3100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.126040\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.134142\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.073746\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.108039\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.096637\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.140294\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.222685\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.156415\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.105521\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.128152\n",
            "\n",
            "Test set: Average loss: 0.0908, Accuracy: 9737/10000 (97.3700%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.134063\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.109515\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.332245\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.114256\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.136425\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.095057\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.195310\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.084777\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.135337\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.133361\n",
            "\n",
            "Test set: Average loss: 0.0886, Accuracy: 9736/10000 (97.3600%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.325184\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.885601\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.989044\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.397669\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.440426\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.478441\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.326770\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.310250\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.301229\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148094\n",
            "\n",
            "Test set: Average loss: 0.1893, Accuracy: 9423/10000 (94.2300%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.238506\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.301855\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.513924\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.318250\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.183651\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.302089\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.403606\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.278166\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.247415\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.268305\n",
            "\n",
            "Test set: Average loss: 0.1526, Accuracy: 9578/10000 (95.7800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.110649\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.220445\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.590051\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.407042\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.363784\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.174023\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.215548\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.142146\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.270517\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.224515\n",
            "\n",
            "Test set: Average loss: 0.1393, Accuracy: 9603/10000 (96.0300%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.308114\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.177304\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.148845\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.201957\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.228223\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.092475\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.110758\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.206161\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.212925\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.082505\n",
            "\n",
            "Test set: Average loss: 0.1328, Accuracy: 9634/10000 (96.3400%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.055423\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.166821\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.250007\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.154210\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.086089\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.232147\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.026353\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.073919\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.066994\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.377091\n",
            "\n",
            "Test set: Average loss: 0.1303, Accuracy: 9656/10000 (96.5600%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.215370\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.209273\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.122259\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.043258\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.162325\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.051218\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.118277\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.227939\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.162226\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.092581\n",
            "\n",
            "Test set: Average loss: 0.1118, Accuracy: 9714/10000 (97.1400%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.116740\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.080479\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.104473\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.172178\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.065927\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.108897\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.105561\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.198397\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.063626\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.094482\n",
            "\n",
            "Test set: Average loss: 0.1134, Accuracy: 9722/10000 (97.2200%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.070794\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.113153\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.164238\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.071697\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.057837\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.089597\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.096175\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.063011\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.094263\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.121007\n",
            "\n",
            "Test set: Average loss: 0.1012, Accuracy: 9752/10000 (97.5200%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.074852\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.385688\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.031501\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.136638\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.159492\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.177426\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.250406\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.040782\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.260724\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.068618\n",
            "\n",
            "Test set: Average loss: 0.1374, Accuracy: 9708/10000 (97.0800%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.106467\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.088402\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.059749\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.128330\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.112931\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.055924\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.164468\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.095333\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.103071\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.068601\n",
            "\n",
            "Test set: Average loss: 0.1204, Accuracy: 9735/10000 (97.3500%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.492369\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.639072\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.722723\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.325563\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.249158\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.318726\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.267369\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.210668\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.417765\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.186516\n",
            "\n",
            "Test set: Average loss: 0.1273, Accuracy: 9621/10000 (96.2100%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.389127\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.317828\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.433494\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.206181\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.137080\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.282639\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.384472\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.190331\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.284172\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.193937\n",
            "\n",
            "Test set: Average loss: 0.1047, Accuracy: 9698/10000 (96.9800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.257220\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.373090\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.483101\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.374547\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.257727\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.170573\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.240231\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.145272\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.226155\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.154952\n",
            "\n",
            "Test set: Average loss: 0.0939, Accuracy: 9733/10000 (97.3300%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.146224\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.172515\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.110545\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.220831\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.296631\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.087349\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.205676\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.153866\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.291579\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.152928\n",
            "\n",
            "Test set: Average loss: 0.0918, Accuracy: 9753/10000 (97.5300%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.048978\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.149493\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.183375\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.115636\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.070246\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.179962\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.025133\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.131229\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.117474\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.169334\n",
            "\n",
            "Test set: Average loss: 0.0810, Accuracy: 9780/10000 (97.8000%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.160065\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.056339\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.101600\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.065687\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.146902\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.073029\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.333461\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.145311\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.107867\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.085833\n",
            "\n",
            "Test set: Average loss: 0.0839, Accuracy: 9787/10000 (97.8700%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.142453\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.027027\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.168041\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.043032\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.055245\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.061601\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.056993\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.127426\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.109240\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.170625\n",
            "\n",
            "Test set: Average loss: 0.0779, Accuracy: 9796/10000 (97.9600%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.141185\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.111479\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.146496\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.087994\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.104067\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.038620\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.036394\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.117344\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078699\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.184236\n",
            "\n",
            "Test set: Average loss: 0.0713, Accuracy: 9801/10000 (98.0100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.039220\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.222066\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.086934\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.232004\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.123284\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.172573\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.396936\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.022333\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.235871\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.085450\n",
            "\n",
            "Test set: Average loss: 0.0793, Accuracy: 9793/10000 (97.9300%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.034740\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.067164\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.101742\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.163771\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.185959\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.107373\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.105795\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.079622\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.058469\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.203151\n",
            "\n",
            "Test set: Average loss: 0.0683, Accuracy: 9815/10000 (98.1500%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRc73jUPqF5B"
      },
      "source": [
        "## Momentum \n",
        "\n",
        "Momentum in gradient descent is similar to the concept of momentum in physics. The optimization process resembles a ball rolling down the hill. Momentum keeps the ball moving in the same direction that it is already moving in. The gradient can be thought of as a force pushing the ball in some other direction.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/640/1*i1Qc2E0TVlPHEKG7LepXgA.gif\">\n",
        "</p>\n",
        "\n",
        "Mathematically it can be expressed as follows-\n",
        "$$w_{t+1}=w_t-\\eta (\\nabla f(w_t) +\\beta m_{t}) $$\n",
        "$$m_{t+1}= \\nabla f(w_t) +\\beta m_{t}$$\n",
        "or, equivalently\n",
        "$$w_{t+1}= w_t -\\eta\\nabla f(w_t) +\\beta (w_{t} -w_{t-1})$$\n",
        "\n",
        "where\n",
        "*   $m$ is the momentum (the running average of the past gradients, initialized at zero),\n",
        "*   $\\beta\\in [0,1)$ is the damping factor, usually $0.9$ or $0.99$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSYiHGpxHill"
      },
      "source": [
        "\n",
        "\n",
        "Letâ€™s consider two extreme cases to understand this decay rate parameter better. If the decay rate is 0, then it is exactly the same as (vanilla) gradient descent (blue ball). If the decay rate is 1 (and provided that the learning rate is reasonably small), then it rocks back and forth endlessly like the frictionless ball we saw previously; you do not want that. Typically the decay rate is chosen around 0.8â€“0.9 â€” itâ€™s like a surface with a little bit of friction so it eventually slows down and stops (purple ball).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img width=\"460\" height=\"300\" src=\"https://miro.medium.com/max/800/1*zVi4ayX9u0MQQwa90CnxVg.gif\">\n",
        "</p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z03cu7Fd9bSc"
      },
      "source": [
        "#useful link: \n",
        "#https://distill.pub/2017/momentum/"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fueewl41I-7"
      },
      "source": [
        "In the standard SGD formulation, every weight in network is updated with the same learning rate (global $\\eta$). Here, we adapt a learning rate for each weight individually, using information we get from their gradients.\n",
        "\n",
        "## Adagrad\n",
        "\n",
        "Adagrad adapts the learning rate of each parameter, downweighting the learning rates for parameters that have changed a lot and upweighting the learning rates of parameters that have changed very little.\n",
        "\n",
        "It uses a different learning rate for every parameter $w_j$ at every time step, $t$. (The time step here in practice is a minibatch, with everything averaged over that minibatch.) The update for every parameter $w_j$ at each time step (or epoch) $t$ then becomes\n",
        "\n",
        "$$w_{t+1}=w_t- \\frac{\\eta}{\\sqrt{v_{t+1}+\\epsilon}} \\nabla f(w_t)$$\n",
        "\n",
        "where the equation holds for every feature $w_j$ separately. Thus, $\\nabla f(w_{t})$ is the partial derivative of the objective function w.r.t. to the parameter $w_j$ at time step $t$ and the learning rate for each feature is scaleed using the sum of the gradients for that feature:\n",
        "\n",
        "$$v_{t+1} = \\sum^t_{\\tau=1} \\nabla f(w_{\\tau})^2$$\n",
        "\n",
        "Adagrad effectively selects low learning rates for parameters associated with frequently occurring features, and high learning rates for parameters associated with infrequent features. It is thus well-suited for dealing with sparse data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49-vUMw91QDw"
      },
      "source": [
        "## RMSprop\n",
        "\n",
        "RMSprop seeks to reduce Adagrad's aggressive, monotonically decreasing learning rate. Instead of accumulating all past squared gradients, RMSprop restricts the window of accumulated past gradients to some fixed size. The sum of gradients is recursively defined as a decaying average of all past squared gradients.\n",
        "\n",
        "$$w_{t+1}=w_t- \\frac{\\eta}{\\sqrt{v_{t+1}+\\epsilon}} \\nabla f(w_t)$$\n",
        "$$v_{t+1}=\\alpha v_t+(1-\\alpha)(\\nabla f(w_t))^2$$\n",
        "\n",
        "where \n",
        "*   $v$ is the 2nd moment estimate which depends (as a fraction $\\alpha$ similarly to the Momentum term) on the previous average and the current gradient.\n",
        "*   $\\alpha$ is usually set to $0.9$, while a good default value for the learning rate $\\eta$ is $0.001$.\n",
        "\n",
        "We update $v$ to estimate this noisy quantity via an exponential moving average (which is a standard way of maintaining an average of a quantity that may change over time). We need to put larger weights on the newer values as they provide more information. One way to do that is down-weight old values exponentially. The values in the $v$ calculation that are very old are down-weighted at each step by an $\\alpha$ constant, which varies between 0 and 1. This dampens the old values until they are no longer an important part of the exponential moving average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFaBE3Dg4xEv"
      },
      "source": [
        "## Adam\n",
        "\n",
        "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp (described in lecture) and Momentum. \n",
        "\n",
        "**How does Adam work?**\n",
        "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
        "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
        "3. It updates parameters in a direction based on combining information from \"1\" and \"2\".\n",
        "\n",
        "The update rule is, for $l = 1, ..., L$: \n",
        "\n",
        "$$\\begin{cases}\n",
        "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
        "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
        "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
        "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
        "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
        "\\end{cases}$$\n",
        "where:\n",
        "- t counts the number of steps taken of Adam \n",
        "- L is the number of layers\n",
        "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
        "- $\\alpha$ is the learning rate\n",
        "- $\\varepsilon$ is a very small number to avoid dividing by zero\n",
        "\n",
        "As usual, we will store all parameters in the `parameters` dictionary  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgdI1x0MS8-D"
      },
      "source": [
        "## Learn and compare different adaptive learning rate optimizers\n",
        "\n",
        "For SGD with fixed schedule, Adagrad, RMSprop, Adam, how do they differ on train and test error? Which one works the best?\n",
        "\n",
        "We compare these optimizers by performing digit classification task in MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIWmVS4TzfC5"
      },
      "source": [
        "def optimizer_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    if args['optimizer'] == 'sgd':\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'adagrad':\n",
        "        optimizer = optim.Adagrad(model.parameters(), lr=args['lr'])\n",
        "    elif args['optimizer'] == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=1e-3)\n",
        "    elif args['optimizer'] == 'adam':\n",
        "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        train_acc = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_list.append(100.-train_acc)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        test_list.append(100.-test_acc)\n",
        "\n",
        "    return train_list, test_list "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uktQPea8gNBk"
      },
      "source": [
        "The training takes over 20 mins. Please skip running below cells for now and come back if time is allowed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3yrm_IMB3u3"
      },
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'anneal_type': 'linear',\n",
        "        'optimizer': 'sgd',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEbJfEL-B4xO"
      },
      "source": [
        "optimizer = ['sgd', 'adagrad', 'rmsprop', 'adam']\n",
        "error_dict = {}\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvX9ezmg0Er3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb38c0c-7cf7-482d-b001-442e031adf03"
      },
      "source": [
        "\n",
        "for i in range(len(optimizer)):\n",
        "    args['optimizer'] = optimizer[i]\n",
        "    train_error, test_error = optimizer_eval(args)\n",
        "    error_dict['train' + str(optimizer[i])] = train_error\n",
        "    error_dict['test' + str(optimizer[i])] = test_error"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.032837\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.629122\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.694035\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.588824\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.430997\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.235259\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.327051\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.438414\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.240641\n",
            "\n",
            "Test set: Average loss: 0.3242, Accuracy: 9083/10000 (90.8300%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.242436\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.243902\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.227456\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.385937\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.317543\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.389771\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.224058\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.233456\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.235209\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.212870\n",
            "\n",
            "Test set: Average loss: 0.2650, Accuracy: 9251/10000 (92.5100%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.298318\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.232883\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.086087\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.279249\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.350071\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.158839\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.420400\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.147905\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.219345\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.225677\n",
            "\n",
            "Test set: Average loss: 0.2318, Accuracy: 9356/10000 (93.5600%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.142316\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.313546\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.178308\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.287100\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.140380\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.185279\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.288050\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.242471\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.189803\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.215473\n",
            "\n",
            "Test set: Average loss: 0.2076, Accuracy: 9412/10000 (94.1200%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.182003\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.234314\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.254971\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.183941\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.106989\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.147235\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.153088\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.086903\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.223234\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.239446\n",
            "\n",
            "Test set: Average loss: 0.1874, Accuracy: 9452/10000 (94.5200%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.089253\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.100532\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.189340\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.177487\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.233099\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.075876\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.211649\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.128301\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.225081\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.153328\n",
            "\n",
            "Test set: Average loss: 0.1735, Accuracy: 9492/10000 (94.9200%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.109130\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.175746\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.199215\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.135223\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.133749\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.182931\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.148826\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.114926\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.251422\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.212851\n",
            "\n",
            "Test set: Average loss: 0.1586, Accuracy: 9545/10000 (95.4500%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.178132\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.145684\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.320427\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.138648\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.278651\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.059940\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.168369\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.140632\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.182406\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.113123\n",
            "\n",
            "Test set: Average loss: 0.1479, Accuracy: 9561/10000 (95.6100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.121921\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.204191\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.066452\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.113844\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.128602\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.126290\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.173018\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.147866\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.114426\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.213025\n",
            "\n",
            "Test set: Average loss: 0.1395, Accuracy: 9580/10000 (95.8000%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.168869\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.147322\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.212809\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.043461\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.050419\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.039792\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.178710\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.126753\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.146716\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.074096\n",
            "\n",
            "Test set: Average loss: 0.1304, Accuracy: 9617/10000 (96.1700%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.388890\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.366703\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.377505\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.376768\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.196248\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.132494\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.184106\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.220852\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148606\n",
            "\n",
            "Test set: Average loss: 0.1632, Accuracy: 9509/10000 (95.0900%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.121920\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.100396\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.151241\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.207345\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.232248\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.137821\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.124586\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.080439\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.113142\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.080849\n",
            "\n",
            "Test set: Average loss: 0.1261, Accuracy: 9624/10000 (96.2400%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.098819\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.150247\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.019231\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.220448\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.159817\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.049981\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.103945\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.066321\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.153853\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.119005\n",
            "\n",
            "Test set: Average loss: 0.1118, Accuracy: 9659/10000 (96.5900%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.072694\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.126298\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.062964\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.115581\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.060101\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.073408\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.059507\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.122525\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.066853\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.054285\n",
            "\n",
            "Test set: Average loss: 0.1030, Accuracy: 9685/10000 (96.8500%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.096208\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.096870\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.164528\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.051003\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.057790\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.084285\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.066750\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.029086\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.116548\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.090651\n",
            "\n",
            "Test set: Average loss: 0.0976, Accuracy: 9708/10000 (97.0800%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.032824\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.035013\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.079372\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.112135\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.102520\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.055589\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.089765\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.047423\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.099090\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.091278\n",
            "\n",
            "Test set: Average loss: 0.0940, Accuracy: 9701/10000 (97.0100%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.042690\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.046488\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.093940\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.032782\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.076099\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.054453\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.044213\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.019114\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.131075\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.097819\n",
            "\n",
            "Test set: Average loss: 0.0898, Accuracy: 9720/10000 (97.2000%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.102859\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.054294\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.123099\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.038525\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.106106\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.026133\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.101296\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.071396\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.117068\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.074890\n",
            "\n",
            "Test set: Average loss: 0.0862, Accuracy: 9731/10000 (97.3100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.064732\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.067163\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.038612\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.043599\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.020494\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.056184\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.057058\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.094927\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.064371\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.107810\n",
            "\n",
            "Test set: Average loss: 0.0833, Accuracy: 9743/10000 (97.4300%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.111037\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.039862\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.090537\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.012150\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.019872\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.016688\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.103043\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.038929\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.082485\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.041505\n",
            "\n",
            "Test set: Average loss: 0.0805, Accuracy: 9743/10000 (97.4300%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.385785\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.371213\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.341067\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.344548\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.162885\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.140098\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.129166\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.155162\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.135359\n",
            "\n",
            "Test set: Average loss: 0.1613, Accuracy: 9506/10000 (95.0600%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.104546\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.062732\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.180969\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.218140\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.266680\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.109268\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.101909\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.076703\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.081619\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.062462\n",
            "\n",
            "Test set: Average loss: 0.0933, Accuracy: 9728/10000 (97.2800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.046463\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.083771\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.005789\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.159062\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.084464\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.010131\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.033753\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.075555\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.069895\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.061919\n",
            "\n",
            "Test set: Average loss: 0.0859, Accuracy: 9739/10000 (97.3900%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029016\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.043269\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.019946\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.059619\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.024886\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.084934\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.038397\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.059965\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.124753\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.052197\n",
            "\n",
            "Test set: Average loss: 0.0931, Accuracy: 9705/10000 (97.0500%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.036322\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.062055\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.065620\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.013747\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.058700\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.118284\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.051109\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.022339\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.102558\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.051937\n",
            "\n",
            "Test set: Average loss: 0.0757, Accuracy: 9768/10000 (97.6800%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.035705\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.006938\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.014678\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.070237\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.033822\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.021955\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.045731\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.022959\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.113325\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.077432\n",
            "\n",
            "Test set: Average loss: 0.0823, Accuracy: 9760/10000 (97.6000%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.006411\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.023256\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.012609\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.006475\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.052645\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.011838\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.016833\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.003876\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.026978\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.026318\n",
            "\n",
            "Test set: Average loss: 0.0901, Accuracy: 9751/10000 (97.5100%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.034559\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.007882\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.013443\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.003461\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.087243\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003321\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.047332\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.080658\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.022353\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.022222\n",
            "\n",
            "Test set: Average loss: 0.0956, Accuracy: 9755/10000 (97.5500%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.056048\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.007052\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.011989\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.032678\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001490\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.019415\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.001309\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.018987\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.079766\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.051378\n",
            "\n",
            "Test set: Average loss: 0.0887, Accuracy: 9778/10000 (97.7800%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.055464\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.001609\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.071426\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000384\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.003577\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.005469\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002154\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.016758\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.004163\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.005984\n",
            "\n",
            "Test set: Average loss: 0.0936, Accuracy: 9769/10000 (97.6900%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.316347\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.436042\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.379045\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.379335\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.333993\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.188594\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.138908\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.169035\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.235600\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.122576\n",
            "\n",
            "Test set: Average loss: 0.1447, Accuracy: 9565/10000 (95.6500%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.076999\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.078897\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.138241\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.209436\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.250184\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.135194\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.110702\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.082504\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.114068\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.032705\n",
            "\n",
            "Test set: Average loss: 0.0959, Accuracy: 9716/10000 (97.1600%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.082169\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.059701\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.011954\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.118591\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.098541\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.034463\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.071606\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.058951\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.157768\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.079066\n",
            "\n",
            "Test set: Average loss: 0.0847, Accuracy: 9742/10000 (97.4200%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.021069\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.035579\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.037867\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.049115\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029404\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.046746\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.028621\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.088116\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.074260\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.012938\n",
            "\n",
            "Test set: Average loss: 0.0829, Accuracy: 9749/10000 (97.4900%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.020238\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.059519\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.025095\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.011146\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.054091\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.125797\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.048083\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.020759\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.087087\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.071970\n",
            "\n",
            "Test set: Average loss: 0.0916, Accuracy: 9723/10000 (97.2300%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.047986\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.010186\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.023191\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.062690\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.036634\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.009395\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.031833\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.014514\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.165688\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.053050\n",
            "\n",
            "Test set: Average loss: 0.0850, Accuracy: 9741/10000 (97.4100%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.006147\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.031746\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.005073\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.018088\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.023128\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.008727\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.027169\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.002885\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.105008\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.032582\n",
            "\n",
            "Test set: Average loss: 0.1059, Accuracy: 9698/10000 (96.9800%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.012444\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.036440\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.007595\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.001512\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.062504\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.006339\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.076421\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.045525\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.053443\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.003725\n",
            "\n",
            "Test set: Average loss: 0.0945, Accuracy: 9730/10000 (97.3000%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.034892\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.069992\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.003856\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.019828\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001996\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.014289\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.003166\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.040821\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.014823\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.074559\n",
            "\n",
            "Test set: Average loss: 0.1024, Accuracy: 9748/10000 (97.4800%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.016015\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.003711\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.064445\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000375\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.001288\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.003491\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.006884\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.039022\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.008123\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.034631\n",
            "\n",
            "Test set: Average loss: 0.0911, Accuracy: 9754/10000 (97.5400%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JDWUE09040_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c7cacad2-4bb9-401a-b50f-4b7c4cdc5f09"
      },
      "source": [
        "  fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
        "  axs[0].plot(error_dict['trainsgd'], label='SGD', color='b')\n",
        "  axs[1].plot(error_dict['testsgd'], label='SGD', color='b', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainadagrad'], label='Adagrad', color='r')\n",
        "  axs[1].plot(error_dict['testadagrad'], label='Adagrad', color='r', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainrmsprop'], label='RMSprop', color='g')\n",
        "  axs[1].plot(error_dict['testrmsprop'], label='RMSprop', color='g', linestyle='dashed')\n",
        "  axs[0].plot(error_dict['trainadam'], label='Adam', color='orange')\n",
        "  axs[1].plot(error_dict['testadam'], label='Adam', color='orange', linestyle='dashed')\n",
        "  axs[0].set_title('Train')\n",
        "  axs[1].set_title('Test')\n",
        "  axs[0].set_ylabel('Error (%)')\n",
        "  #plt.yscale('log')\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[0].legend()\n",
        "  axs[1].legend()\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4KsR565gkk9"
      },
      "source": [
        "Plot the train and test classification error curves of different optimizers by running below cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuzyGTvyXuhF"
      },
      "source": [
        "## Learning rate scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5G3RimhHSZ52"
      },
      "source": [
        "If the learning rate is too large, optimization diverges; if it is too small, it takes too long to train or we end up with a suboptimal result. People often start large learning rate and then 'decay' or 'anneal'(decrease) it.  This can help both optimization and generalization.\n",
        "\n",
        "Common beliefs in how annealing works come from the optimization analysis of stochastic gradient descent: \n",
        "\n",
        "1.   An initial large learning rate accelerates training or helps the network escape spurious local minima\n",
        "2.   Decaying the learning rate helps the network converge to a local minimum and avoid oscillation. \n",
        "\n",
        "The simplest learning rate schedule is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process. There are other schedules such as square root and exponential decay. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP_zfGmXSd5D"
      },
      "source": [
        "### Compare different annealing schedules: constant, linear, sqrt(t) and exp(-t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jaja6Shbzpb"
      },
      "source": [
        "Firstly, let's plot the simulation of different annealing scheduels: constant, linear, sqrt(t) and exp(-t) in below cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlS2oqhznc7R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "52a8ac96-98f8-4f41-9897-632fc0692d80"
      },
      "source": [
        "model = torch.nn.Linear(2, 1)\n",
        "lr_anneal = ['constant', 'linear', 'sqrt', 'exp']\n",
        "lr_dict = defaultdict(list)\n",
        "\n",
        "for idx in range(len(lr_anneal)):\n",
        "    optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "    if lr_anneal[idx] == 'constant':\n",
        "        lambda1 = lambda epoch: 1\n",
        "    elif lr_anneal[idx] == 'linear':\n",
        "        lambda1 = lambda epoch: max(1e-7, 1 - 0.1*epoch)\n",
        "    elif lr_anneal[idx] == 'sqrt':\n",
        "        lambda1 = lambda epoch: (epoch + 1.0) ** -0.5\n",
        "    elif lr_anneal[idx] == 'exp':\n",
        "        lambda1 = lambda epoch: 0.1 ** epoch\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "    for i in range(10):\n",
        "        optimizer.step()\n",
        "        lr_dict[lr_anneal[idx]].append(optimizer.param_groups[0][\"lr\"])\n",
        "        scheduler.step()\n",
        "\n",
        "\n",
        "plt.plot(range(10), lr_dict['constant'], label='Constant')\n",
        "plt.plot(range(10), lr_dict['linear'], label='Linear')\n",
        "plt.plot(range(10), lr_dict['sqrt'], label='Sqrt')\n",
        "plt.plot(range(10), lr_dict['exp'], label='Exp')\n",
        "plt.title('Annealing Schedules')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUVZ7u7Occlg"
      },
      "source": [
        "Now, check your assumption by running below digit classification example with different learning rate scheduelers: linear, sqrt(t) and exp(-t)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKfrDA0jf1eb"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.fc3 = nn.Linear(784, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0ej3aHf2hA"
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    avg_loss, correct = (0., 0.)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        avg_loss += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % args['log_interval'] == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "    avg_loss /= len(train_loader.dataset)\n",
        "    return 100. * correct / len(train_loader.dataset)\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSKfElhlf6Cd"
      },
      "source": [
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.4f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlmg9nWggwTs"
      },
      "source": [
        "def schedular_eval(args):\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(args['seed'])\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    train_kwargs = {'batch_size': args['batch_size']}\n",
        "    test_kwargs = {'batch_size': args['test_batch_size']}\n",
        "    if use_cuda:\n",
        "        cuda_kwargs = {'num_workers': 1,\n",
        "                       'pin_memory': True,\n",
        "                       'shuffle': True}\n",
        "        train_kwargs.update(cuda_kwargs)\n",
        "        test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "    train_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform),**train_kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(datasets.MNIST('../data', train=False,\n",
        "                       transform=transform), **test_kwargs)\n",
        "\n",
        "    model = Net().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    if args['anneal_type'] == 'constant':\n",
        "        lambda1 = lambda epoch: 1\n",
        "    elif args['anneal_type'] == 'linear':\n",
        "        lambda1 = lambda epoch: max(1e-7, 1 -0.1 * epoch)\n",
        "    elif args['anneal_type'] == 'sqrt':\n",
        "        lambda1 = lambda epoch: (epoch + 1.0) ** -0.5\n",
        "    elif args['anneal_type'] == 'exp':\n",
        "        lambda1 = lambda epoch: 0.1 ** epoch\n",
        "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "\n",
        "    train_list, test_list = [], []\n",
        "    for epoch in range(1, args['epochs'] + 1):\n",
        "        '''\n",
        "        if epoch > 1:\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] *= 0.1\n",
        "        '''\n",
        "        train_acc = train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_list.append(100.-train_acc)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        test_list.append(100.-test_acc)\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_list, test_list "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tBDX8ECdGP_"
      },
      "source": [
        "The training takes over 20 mins. Please skip running below cells for now and come back if time is allowed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijVxIAmmf_r-"
      },
      "source": [
        "# Training settings\n",
        "args = {'batch_size': 64,\n",
        "        'test_batch_size': 1000,\n",
        "        'epochs': 10,\n",
        "        'lr': 0.01,\n",
        "        'momentum': 0.9,\n",
        "        'net_type': 'Net',\n",
        "        'anneal_type': 'linear',\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bXmiXVBgBTp"
      },
      "source": [
        "lr_anneal = ['constant', 'linear', 'sqrt', 'exp']\n",
        "error_dict = {}\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pbs3P3JdrveH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fb2f166-0c80-447e-cf2a-4da54de1e0ce"
      },
      "source": [
        "for i in range(len(lr_anneal)):\n",
        "    args['anneal_type'] = lr_anneal[i]\n",
        "    train_error, test_error = schedular_eval(args)\n",
        "    error_dict['train' + str(lr_anneal[i])] = train_error\n",
        "    error_dict['test' + str(lr_anneal[i])] = test_error"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.349136\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.417331\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.403882\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.375751\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.368692\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175918\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114036\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.154184\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.189471\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.128903\n",
            "\n",
            "Test set: Average loss: 0.1414, Accuracy: 9579/10000 (95.7900%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074819\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.080490\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.177657\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.225769\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.219548\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.166283\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.122383\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.078938\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.149461\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.094920\n",
            "\n",
            "Test set: Average loss: 0.1001, Accuracy: 9708/10000 (97.0800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.077283\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.075826\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.014275\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.121752\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.129635\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.041148\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.089183\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.054590\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.109873\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.073027\n",
            "\n",
            "Test set: Average loss: 0.0848, Accuracy: 9734/10000 (97.3400%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019913\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.064027\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.044192\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.022022\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.057013\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.035551\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.061067\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.075045\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.018925\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.012729\n",
            "\n",
            "Test set: Average loss: 0.0760, Accuracy: 9749/10000 (97.4900%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.043866\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.033281\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.037374\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.014801\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.054460\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.049887\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.046439\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.029228\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.070307\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.116944\n",
            "\n",
            "Test set: Average loss: 0.0706, Accuracy: 9783/10000 (97.8300%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.027633\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.026234\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.025812\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.045322\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.080069\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.013114\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.038063\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.012421\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.035333\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.063068\n",
            "\n",
            "Test set: Average loss: 0.0761, Accuracy: 9763/10000 (97.6300%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.010379\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.040524\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.013731\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.019409\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.066192\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031462\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.014876\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.004106\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.060304\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.036146\n",
            "\n",
            "Test set: Average loss: 0.0779, Accuracy: 9759/10000 (97.5900%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.040603\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.018841\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.014158\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.005372\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.043119\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.005650\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.046900\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.048765\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.028192\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.027868\n",
            "\n",
            "Test set: Average loss: 0.0676, Accuracy: 9791/10000 (97.9100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.026222\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.013068\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.007635\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.031145\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.007217\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.020053\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.003616\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.031104\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.013302\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.034371\n",
            "\n",
            "Test set: Average loss: 0.0646, Accuracy: 9798/10000 (97.9800%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.050196\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.010654\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.031641\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.003748\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.008994\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.004185\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.025229\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.010300\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.020384\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.018640\n",
            "\n",
            "Test set: Average loss: 0.0703, Accuracy: 9784/10000 (97.8400%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.349136\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.417331\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.403882\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.375751\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.368692\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175918\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114036\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.154184\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.189471\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.128903\n",
            "\n",
            "Test set: Average loss: 0.1414, Accuracy: 9579/10000 (95.7900%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074819\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.079555\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.168807\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.222954\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.213742\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.162549\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.118922\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.073307\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.137500\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.094731\n",
            "\n",
            "Test set: Average loss: 0.1015, Accuracy: 9704/10000 (97.0400%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.076175\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.075564\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.015967\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.120521\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.130874\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.041487\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.093015\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.054349\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.115002\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.064764\n",
            "\n",
            "Test set: Average loss: 0.0858, Accuracy: 9735/10000 (97.3500%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.022343\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.071497\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.039645\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.028510\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.054959\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.031655\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.056301\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.069961\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.017832\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.014786\n",
            "\n",
            "Test set: Average loss: 0.0782, Accuracy: 9747/10000 (97.4700%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.042001\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.033148\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.051727\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.020960\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.050841\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.036730\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.049024\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.027650\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.076500\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.118494\n",
            "\n",
            "Test set: Average loss: 0.0713, Accuracy: 9779/10000 (97.7900%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.021602\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.033050\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.039098\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.060581\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.067444\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.017371\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.040132\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.016206\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.034977\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.064930\n",
            "\n",
            "Test set: Average loss: 0.0705, Accuracy: 9777/10000 (97.7700%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.014703\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.020111\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.019937\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.020240\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.036902\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.041847\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.016747\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.007651\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.085078\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.035157\n",
            "\n",
            "Test set: Average loss: 0.0684, Accuracy: 9785/10000 (97.8500%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.059205\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.024013\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.033268\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.013802\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.054232\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.005806\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.062144\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.060046\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.042546\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.051322\n",
            "\n",
            "Test set: Average loss: 0.0643, Accuracy: 9799/10000 (97.9900%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.036834\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.013760\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.017799\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.020708\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.014882\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.015845\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.014123\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.030604\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.034674\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.054913\n",
            "\n",
            "Test set: Average loss: 0.0633, Accuracy: 9802/10000 (98.0200%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.074953\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.013771\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.040370\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.004713\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.008669\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.007399\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.042440\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.017316\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.020547\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.020033\n",
            "\n",
            "Test set: Average loss: 0.0626, Accuracy: 9808/10000 (98.0800%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.349136\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.417331\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.403882\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.375751\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.368692\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175918\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114036\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.154184\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.189471\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.128903\n",
            "\n",
            "Test set: Average loss: 0.1414, Accuracy: 9579/10000 (95.7900%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074819\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.077661\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.150988\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.220421\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.201622\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.148987\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.113347\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.077158\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.134386\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.089979\n",
            "\n",
            "Test set: Average loss: 0.1060, Accuracy: 9698/10000 (96.9800%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.078260\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.082807\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.017904\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.129108\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.138485\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.046029\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.099035\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.055129\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.115468\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.069459\n",
            "\n",
            "Test set: Average loss: 0.0883, Accuracy: 9733/10000 (97.3300%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.024925\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.088997\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.042423\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.039739\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.053312\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.032214\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.063494\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.083579\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.027929\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.019567\n",
            "\n",
            "Test set: Average loss: 0.0819, Accuracy: 9745/10000 (97.4500%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.050111\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.043930\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.076902\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.027424\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.059156\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.039036\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.053732\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.025517\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.084156\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.131091\n",
            "\n",
            "Test set: Average loss: 0.0748, Accuracy: 9770/10000 (97.7000%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.024481\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.032720\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.058012\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.080088\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.078410\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.023490\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.056694\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.020693\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.040286\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.070182\n",
            "\n",
            "Test set: Average loss: 0.0744, Accuracy: 9772/10000 (97.7200%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.021305\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.024760\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.029370\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.026752\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.043552\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.043295\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.024446\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.009746\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.107313\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.046529\n",
            "\n",
            "Test set: Average loss: 0.0716, Accuracy: 9781/10000 (97.8100%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.069280\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.031172\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.064290\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.020294\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.076952\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.008802\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.071729\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.064500\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.054449\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.057680\n",
            "\n",
            "Test set: Average loss: 0.0676, Accuracy: 9781/10000 (97.8100%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.045413\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.022061\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.020460\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.033690\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.022670\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.024608\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.019582\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.050091\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.043591\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.057102\n",
            "\n",
            "Test set: Average loss: 0.0664, Accuracy: 9801/10000 (98.0100%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.084995\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.016901\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.052917\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.006789\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.013939\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.009651\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.062246\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.021866\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.026999\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.027539\n",
            "\n",
            "Test set: Average loss: 0.0672, Accuracy: 9786/10000 (97.8600%)\n",
            "\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.349136\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.417331\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.403882\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.375751\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.368692\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.175918\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.114036\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.154184\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.189471\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.128903\n",
            "\n",
            "Test set: Average loss: 0.1414, Accuracy: 9579/10000 (95.7900%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074819\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.068854\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.105536\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.200031\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.178148\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.128395\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.116763\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.068769\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.133179\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.093464\n",
            "\n",
            "Test set: Average loss: 0.1193, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.084417\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.117839\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.021795\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.179207\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.156148\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.056993\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.124094\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.062400\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.170660\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.111491\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9654/10000 (96.5400%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.058720\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.167197\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.074768\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.123552\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.071485\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.066323\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.114356\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.144739\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.068685\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.072622\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.103067\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.109223\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.192052\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.068193\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.080822\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.079057\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.086134\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.042912\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.167041\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.152210\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.058905\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.047016\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.152365\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.118457\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.160482\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.056728\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.159930\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.055013\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.111069\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.098981\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.088596\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.101010\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.156396\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.100377\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.084232\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.112573\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.089085\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.046800\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.189412\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.154774\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.140075\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.090562\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.257334\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.083269\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.217311\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.038397\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.128885\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.113642\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.140670\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.107499\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.123261\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.136390\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.047292\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.088708\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.119613\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.094765\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.177066\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.144644\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.097278\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.200718\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.119532\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.098227\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.174111\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.026076\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.070466\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.031741\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.174319\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.123618\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.124471\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.088743\n",
            "\n",
            "Test set: Average loss: 0.1184, Accuracy: 9655/10000 (96.5500%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esrdkxkWuQI-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ddc9df9d-a429-4716-dc6e-e0eb49b369da"
      },
      "source": [
        "\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10,4))\n",
        "axs[0].plot(error_dict['trainconstant'], label='Constant', color='b')\n",
        "axs[1].plot(error_dict['testconstant'], label='Constant', color='b', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainlinear'], label='Linear', color='r')\n",
        "axs[1].plot(error_dict['testlinear'], label='Linear', color='r', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainsqrt'], label='Sqrt', color='g')\n",
        "axs[1].plot(error_dict['testsqrt'], label='Sqrt', color='g', linestyle='dashed')\n",
        "axs[0].plot(error_dict['trainexp'], label='Exp', color='orange')\n",
        "axs[1].plot(error_dict['testexp'], label='Exp', color='orange', linestyle='dashed')\n",
        "axs[0].set_title('Train')\n",
        "axs[1].set_title('Test')\n",
        "axs[0].set_ylabel('Error (%)')\n",
        "#plt.yscale('log')\n",
        "axs[0].set_xlabel('Epoch')\n",
        "axs[1].set_xlabel('Epoch')\n",
        "axs[0].legend()\n",
        "axs[1].legend()\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTti3HOi1cWC"
      },
      "source": [
        "## Concluding Optimisation\n",
        "\n",
        "So we've seen a bunch of different optimizing methods, Batch Normalisation layers, and annealing methods which control the way the optimisation tapers off. Usually, we focus on optimisation technique and fine tuning after we've decided on the model we want to use on the data, though it is worth keeping an eye on the general optimisation techniques when we are training (such as the kind of optimiser we will be choosing). Optimisation works in tandem with the second set of topics we will be exploring, Regularisation, in part of being the key ways we truly master training a model. Model architecture and deciding the right kind of model for your data is crucial, but optimisation and regularisation is what it make the model ready for production (if that is your goal!).\n",
        "\n",
        "So while optimisation dealt with the way our model back propagates information about changing the model weights based on the predictions, *regularisation* deals with ways we can allow our model to generalise better, and avoid over-fitting our model, and allowing it to be flexible in the way it deals with new data. In the following section, we will deal with multiple ways to regularise our data. We will continue dealing with image based models, but will move to a different dataset which is more complex, of animal faces.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oy4MWLlGFRzD"
      },
      "source": [
        "## Keras Options\n",
        "\n",
        "Here are set of links and resources for optimisation with Keras.\n",
        "\n",
        "[Keras Optimizer Documentation](https://keras.io/api/optimizers/)\n",
        "\n",
        "[Blog post on Keras optimizer options](https://machinelearningknowledge.ai/keras-optimizers-explained-with-examples-for-beginners/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_QkFfs8bGM3"
      },
      "source": [
        "# Regularisation\n",
        "\n",
        "Useful links:\n",
        "\n",
        "- [Chapter 7, deep learning book](https://www.deeplearningbook.org/contents/regularization.html)\n",
        "- [Slides and Code for PyTorch regularisation from Sebastian Raschka](https://github.com/rasbt/stat479-deep-learning-ss19/tree/master/L10_regularization)\n",
        "- [towards data science blog post](https://towardsdatascience.com/regularization-in-deep-learning-l1-l2-and-dropout-377e75acc036)\n",
        "- [medium blog post](https://medium.com/@dhartidhami/regularization-in-deep-learning-2065b7c889e5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvXxa4b2l4m4"
      },
      "source": [
        "## Setup\n",
        "Note that some of the code for today can take up to an hour to run. We have therefore \"hidden\" that code and shown the resulting outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrBw60zoMRqr"
      },
      "source": [
        "#@title Import functions\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import pathlib\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from IPython.display import HTML, display"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDBtaMET-fNA"
      },
      "source": [
        "# @title Figure Settings\n",
        "import ipywidgets as widgets\n",
        "%matplotlib inline \n",
        "fig_w, fig_h = (8, 6)\n",
        "plt.rcParams.update({'figure.figsize': (fig_w, fig_h)})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "SMALL_SIZE = 12\n",
        "\n",
        "plt.rcParams.update(plt.rcParamsDefault)\n",
        "plt.rc('animation', html='jshtml')\n",
        "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
        "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
        "plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n",
        "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
        "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
        "plt.rc('figure', titlesize=SMALL_SIZE)  # fontsize of the figure title"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lf_iMdDtkH0"
      },
      "source": [
        "# @title Loading Animal Faces data\n",
        "%%capture\n",
        "!rm -r AnimalFaces32x32/\n",
        "!git clone https://github.com/arashash/AnimalFaces32x32\n",
        "!rm -r afhq/\n",
        "!unzip ./AnimalFaces32x32/afhq_32x32.zip"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvhx88j7e6m2"
      },
      "source": [
        "# @title Loading Animal Faces Randomized data\n",
        "%%capture\n",
        "!rm -r Animal_faces_random/\n",
        "!git clone https://github.com/Ravi3191/Animal_faces_random.git\n",
        "!rm -r afhq_random_32x32/\n",
        "!unzip ./Animal_faces_random/afhq_random_32x32.zip\n",
        "!rm -r afhq_10_32x32/\n",
        "!unzip ./Animal_faces_random/afhq_10_32x32.zip"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMuvvw3VHm1Z"
      },
      "source": [
        "#@title Seeding for Reproducibility\n",
        "seed = 90108\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.set_deterministic(True)\n",
        "def seed_worker(worker_id):\n",
        "    worker_seed = seed % (worker_id+1)\n",
        "    np.random.seed(worker_seed)\n",
        "    random.seed(worker_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxCVV-2yt-GX"
      },
      "source": [
        "# @title Helper functions\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.axis(False)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSaZl2xSdALk"
      },
      "source": [
        "Now, lets define a Animal Net model, train, test and main functions which we will use quite frequently this week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSfEJun00dwZ"
      },
      "source": [
        "##Network Class - Animal Faces\n",
        "class Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXPoWJta6hs8"
      },
      "source": [
        "The train function takes in the current model along with the train_loader and loss function and updates the parameters for a single pass of the entire dataset. The test function takes in the current model after every epoch and calculates the accuracy on the test dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovAQWp_GMfUa"
      },
      "source": [
        "def train(args, model, device, train_loader, optimizer, epoch,reg_function1=None,reg_function2=None,criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Trains the current inpur model using the data \n",
        "    from Train_loader and Updates parameters for a single pass\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        if reg_function1 is None:\n",
        "            loss = criterion(output, target)\n",
        "        elif reg_function2 is None:\n",
        "            loss = criterion(output, target)+args['lambda']*reg_function1(model)\n",
        "        else:\n",
        "            loss = criterion(output, target)+args['lambda1']*reg_function1(model)+args['lambda2']*reg_function2(model)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CwjQo-zMf71"
      },
      "source": [
        "def test(model, device, test_loader, loader = 'Test',criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Tests the current Model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return 100. * correct / len(test_loader.dataset)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6umnH-Znil7"
      },
      "source": [
        "def main(args, model,train_loader,val_loader,test_data,reg_function1=None,reg_function2=None,criterion=F.nll_loss):\n",
        "    \"\"\"\n",
        "    Trains the model with train_loader and tests the learned model using val_loader\n",
        "    \"\"\"\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu') \n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    val_acc_list, train_acc_list,param_norm_list = [], [], []\n",
        "    for epoch in tqdm(range(args['epochs'])):\n",
        "        train(args, model, device, train_loader, optimizer, epoch,reg_function1=reg_function1,reg_function2=reg_function2)\n",
        "        train_acc = test(model,device,train_loader, 'Train')\n",
        "        val_acc = test(model,device,val_loader, 'Val')\n",
        "        param_norm = calculate_frobenius_norm(model)\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "        param_norm_list.append(param_norm)\n",
        "\n",
        "    return val_acc_list, train_acc_list, param_norm_list, model, 0"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAGlmnIjWrJ7"
      },
      "source": [
        "One way to think about Regularization is to think in terms of the magnitude of the overall weights of the model. A model with big weights can fit more data perfectly. Wheras a model with smaller weights tend to underperform on the train set but can suprisingly do very well on the test set. Too small of a weights can also be as issue an it can the underfit the model.\n",
        "\n",
        "This week we use the sum of Frobenius Norm of all the tensors in the model as a metric to measure the \"size of the model\".\n",
        "\n",
        "### Frobenius Norm\n",
        "Before we start let us do a quick recollection of Frobenius Norm. The Frobenius norm, sometimes also called the Euclidean norm (a term unfortunately also used for the vector $L^2$ norm), is matrix norm of an mÃ—n matrix A defined as the square root of the sum of the absolute squares of its elements.\n",
        "\\begin{equation}\n",
        "||A||_F= \\sqrt(\\sum_{i=1}^m\\sum_{j=1}^n|a_{ij}|^2)\n",
        "\\end{equation} \n",
        "\n",
        "Let's implement this so we have it handy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VzneV22OxbZ",
        "outputId": "0463a63c-5ba4-4d5a-c46c-2375e487afb9"
      },
      "source": [
        "def calculate_frobenius_norm(model):\n",
        "    norm = 0.0\n",
        "\n",
        "    # Sum all the parameters\n",
        "    for param in model.parameters():\n",
        "        norm += torch.sum(param**2)\n",
        "\n",
        "    # Take a square root of the sum of squares of all the parameters\n",
        "    norm = norm**0.5\n",
        "    return norm\n",
        "\n",
        "net = nn.Linear(10,1)\n",
        "print(f'Frobenius Norm of Single Linear Layer: {calculate_frobenius_norm(net)}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Frobenius Norm of Single Linear Layer: 0.44787365198135376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta2T4mGhQu5q"
      },
      "source": [
        "We use the sum of Frobenius Norm of all the tensors in the model as a metric to measure the \"size of the model\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5risoZLh-boy"
      },
      "source": [
        "## Overfitting and Memorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb_o51L8dw6w"
      },
      "source": [
        "Neural networks with a high number of parameters are prone to overfitting on the training data. Overfitting is when we have low bias and high variance - the model is able to model the training data well but generalises poorly.\n",
        "\n",
        "In principle, we should not touch our test set until after we have chosen all our hyperparameters. Were we to use the test data in the model selection process, there is a risk that we might overfit the test data. Then we would be in serious trouble. If we overfit our training data, there is always the evaluation on test data to keep us honest. But if we overfit the test data, how would we ever know?\n",
        "\n",
        "Note that there is another kind of overfitting: you do \"honest\" fitting on one set of images or posts, or medical records, but it may not generalize to other sets of images, posts or medical records.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_Y9eHGXe5-D"
      },
      "source": [
        "### Validation Dataset\n",
        "\n",
        "A common practice to address this problem is to split our data three ways, using a validation dataset (or validation set) to tune the hyperparameters.\n",
        "\n",
        "Ideally we would only touch the test data once, to assess the very best model or to compare a small number of models to each other, real-world test data is seldom discarded after just one use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uutJgdsFAk7Y"
      },
      "source": [
        "### Does a neural network memorize?\n",
        "\n",
        "Given sufficiently large networks and enough training, Neural Networks can acheive almost 100% train accuracy.\n",
        "\n",
        "In this section we train three MLP's one each on:\n",
        "\n",
        "\n",
        "1.   Animal Faces Dataset\n",
        "2.   Completely Noisy Dataset (Random Shuffling of all labels)\n",
        "3.   Partially Noisy Dataset (Random Shuffling of 15% labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4QhI5n_skGC"
      },
      "source": [
        "Now, let's create the required dataloaders for all the three datasets. Take a quick look at how we split the data. We train on a fraction of the dataset as it will be easier to train and also visualize overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBBuMD3vSvT"
      },
      "source": [
        "##Dataloaders for the Dataset\n",
        "batch_size = 128\n",
        "classes = ('cat', 'dog', 'wild')\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))    \n",
        "     ])\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the  Original Dataset\n",
        "\n",
        "\n",
        "img_train_data, img_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "train_loader = torch.utils.data.DataLoader(img_train_data,batch_size=batch_size,worker_init_fn=seed_worker)\n",
        "val_loader = torch.utils.data.DataLoader(img_val_data,batch_size=1000,worker_init_fn=seed_worker)\n",
        "\n",
        "#creating test dataset\n",
        "test_transform = transforms.Compose([\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
        "     ])\n",
        "img_test_dataset = ImageFolder(data_path/'val', transform=test_transform)\n",
        "\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the  Random Dataset\n",
        "\n",
        "#splitting randomized data into training and validation data \n",
        "data_path = pathlib.Path('.')/'afhq_random_32x32/afhq_random' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "random_img_train_data, random_img_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Randomized train and validation dataloader\n",
        "rand_train_loader = torch.utils.data.DataLoader(random_img_train_data,batch_size=batch_size,num_workers = 0, worker_init_fn=seed_worker)\n",
        "rand_val_loader = torch.utils.data.DataLoader(random_img_val_data,batch_size=1000,num_workers = 0,worker_init_fn=seed_worker)\n",
        "\n",
        "####################################################\n",
        "\n",
        "##Dataloaders for the Partially Random Dataset\n",
        "\n",
        "#Splitting data between training and validation dataset for partially randomized data\n",
        "data_path = pathlib.Path('.')/'afhq_10_32x32/afhq_10' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "partially_random_train_data, partially_random_val_data,_ = torch.utils.data.random_split(img_dataset, [100,100,14430])\n",
        "\n",
        "#Training and Validation loader for partially randomized data\n",
        "partial_rand_train_loader = torch.utils.data.DataLoader(partially_random_train_data,batch_size=batch_size,num_workers = 0,worker_init_fn=seed_worker)\n",
        "partial_rand_val_loader = torch.utils.data.DataLoader(partially_random_val_data,batch_size=1000,num_workers = 0,worker_init_fn=seed_worker)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKu5dp7KgJIN"
      },
      "source": [
        "Now let's define a model which has a very high number of parameters when compared with the training data points and train it on all these datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ9HSgxr-f3r"
      },
      "source": [
        "##Network Class - Animal Faces\n",
        "class Big_Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Big_Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 124)\n",
        "        self.fc2 = nn.Linear(124, 64)\n",
        "        self.fc3 = nn.Linear(64, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = F.leaky_relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y75WnHUBOIvF"
      },
      "source": [
        "##Here we have 100 true train data.\n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxNvmMraOKDx"
      },
      "source": [
        "model = Big_Animal_Net()"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrvUOxYBfRVG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "1d149d5d-67d4-4a2e-ff05-9dce18350fe3"
      },
      "source": [
        "start_time = time.time()\n",
        "val_acc_pure, train_acc_pure, _, model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Time to memorize the dataset:\",end_time - start_time)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [00:18<00:00, 10.81it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Time to memorize the dataset: 18.513160705566406\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "image/png": {
              "width": 579,
              "height": 459
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW9z2IggZRKU"
      },
      "source": [
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERwLcdXb8TRw"
      },
      "source": [
        "### Data Visualizer\n",
        "Before we proceed to train the model on a data with random labels, let us visualize and verify for ourselves if the data is random or not. Here, we have classes = (\"cat\",\"dog\",\"wild\"). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seeyR3dUFafW"
      },
      "source": [
        "def Visualize_data(dataloader):\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch Dataloader\n",
        "    It visualizes the images in the dataset and the classes they belong to.\n",
        "  \"\"\"\n",
        "\n",
        "  for idx,(data,label) in enumerate(dataloader):\n",
        "\n",
        "    plt.figure(idx)\n",
        "    #Choose the datapoint you would like to visualize\n",
        "    index = 22\n",
        "    \n",
        "    #choose that datapoint using index and permute the dimensions and bring the pixel values between [0,1]\n",
        "    data = data[index].permute(1,2,0)* torch.tensor([0.5,0.5,0.5]) + torch.tensor([0.5,0.5,0.5])\n",
        "    \n",
        "    #Convert the torch tensor into numpy\n",
        "    data = data.numpy()\n",
        "    \n",
        "    plt.imshow(data)\n",
        "    image_class = classes[label[index].item()]\n",
        "    print(f'The image belongs to : {image_class}')\n",
        "\n",
        "  plt.show()\n",
        "Visualize_data(rand_train_loader)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgdM2MhQSkbp"
      },
      "source": [
        "We can see that the model is mixed up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE9wtARrg6nx"
      },
      "source": [
        "##Here we have 100 completely shuffled train data.\n",
        "args = {'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "\n",
        "\n",
        "val_acc_random, train_acc_random, _,model,_ = main(args,model,rand_train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_random,label='Val Accuracy random',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_random,label='Train Accuracy random',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_random),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9JAPoEWg1bp"
      },
      "source": [
        "Finally lets train on a parially shuffled dataset where 15% of the labels are noisy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf0ovppPnrxs"
      },
      "source": [
        "##Here we have 100 partially shuffled train data.\n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "\n",
        "\n",
        "val_acc_shuffle, train_acc_shuffle, _,_,_ = main(args,model,partial_rand_train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "#train and test acc plot\n",
        "plt.plot(val_acc_shuffle,label='Val Accuracy shuffle',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_shuffle,label='Train Accuracy shuffle',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_shuffle),c = 'green',ls = 'dashed')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCM3xI3DzJ0M"
      },
      "source": [
        "plt.plot(val_acc_pure,label='Val - Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train - Pure',c='red',ls = 'solid')\n",
        "plt.plot(val_acc_random,label='Val - Random',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_random,label='Train - Random',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_shuffle,label='Val 15% shuffle',c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_shuffle,label='Train 15% shuffle',c='green',ls = 'solid')\n",
        "plt.title('Memorization')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSfURqxBwq97"
      },
      "source": [
        "Given that the NN fit/memorize the training data perfectly, Do you think it generalizes well? What makes you think it does or doesn't?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKhJ_nxdaK-7"
      },
      "source": [
        "Isn't it supprising to see that the NN was able to acheive 100% train accuracy on randomly shuffled labels. This is one of the reasons why training accuracy is not a good indicator of model performance. \n",
        "\n",
        "Also it is interesting to note that sometimes the model trained on slightly shuffled data does slightly better than the one trained on pure data.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMLQYrOBII5a"
      },
      "source": [
        "##Early Stopping\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nV3jPkkgW7fN"
      },
      "source": [
        "\n",
        "Now that we have established that the validation accuracy reaches the peak well before the model overfits we want to somehow stop the training early. You should have also observed from the above plots that the train/test loss on real data is not very smooth and hence you might guess that the choice of epoch can play a very large role on the val/test accuracy of your model. \n",
        "\n",
        "Early stopping is a way to end training when the validation accuracies do not increase for over a certain number of epochs. Though this makes sure that we do not overfit on the train data we still haven't solved the problem of local variance. To overcome this we also save the best model based on the val loss/accuracy for use on test dataset.\n",
        "\n",
        "![Overfitting](https://images.deepai.org/glossary-terms/early-stopping-machine-learning-5422207.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjIGMSiiAwQ"
      },
      "source": [
        "The following function figures out the epoch best suited for stopping early."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7MjxZDTKgkz"
      },
      "source": [
        "def early_stopping_main(args,model,train_loader,val_loader,test_data):\n",
        "\n",
        "    \"\"\"\n",
        "        Inputs: \n",
        "            Model: Pytorch model\n",
        "            Loaders: Pytorch Train and Validation loaders\n",
        "        The function trains the model and terminates the training based on the early stopping criterion.\n",
        "    \"\"\"\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    patience = 20\n",
        "    wait = 0\n",
        "\n",
        "    best_acc  = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    val_acc_list, train_acc_list = [], []\n",
        "    for epoch in tqdm(range(args['epochs'])):\n",
        "        train(args, model, device, train_loader, optimizer, epoch)\n",
        "        train_acc = test(model,device,train_loader, 'Train')\n",
        "        val_acc = test(model,device,val_loader, 'Val')\n",
        "        if (val_acc > best_acc):\n",
        "          best_acc = val_acc\n",
        "          best_epoch = epoch\n",
        "          best_model = copy.deepcopy(model)\n",
        "          wait = 0\n",
        "        else:\n",
        "          wait += 1\n",
        "        if (wait > patience):\n",
        "          print('early stopped on epoch:',epoch)\n",
        "          break\n",
        "        train_acc_list.append(train_acc)\n",
        "        val_acc_list.append(val_acc)\n",
        "\n",
        "    return val_acc_list, train_acc_list, best_model, best_epoch\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Na0NM8Dzx5f"
      },
      "source": [
        "args = {'epochs': 200,\n",
        "        'lr': 5e-4,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_pure, train_acc_pure,_,_ ,best_epoch = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "model = Animal_Net()\n",
        "val_acc_earlystop, train_acc_earlystop,_,best_epoch = early_stopping_main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "print(\"Maximum Validation Accuracy is reached at epoch:%2d\"%(best_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3vFW5Af78sf"
      },
      "source": [
        "Do you think Early stopping can be harmful for the training of your network?Discuss among your pod why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlj0-w2r-PIx"
      },
      "source": [
        "##L1/LASSO Regularization\n",
        "\n",
        "Some of you might have already come across L1 and L2 regularization before in other courses. L1 and L2 are the most common types of regularization. These update the general cost function by adding another term known as the regularization term.\n",
        "\n",
        "***Cost function = Loss (say, binary cross entropy) + Regularization term***\n",
        "\n",
        "Due to the addition of this regularization term, the values of parameters decrease because it assumes that a neural network with a lower parameter values leads to simpler models. Therefore, it will also reduce overfitting to quite an extent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-TABViwyd7P"
      },
      "source": [
        "L1 Regularization uses a Regularization Function which is the sum of the absolute value of all the weights in DLN, resulting in the following loss function ( L  is the usual Cross Entropy loss):\n",
        "\n",
        "\\begin{equation}\n",
        "L_R=L+Î»âˆ‘|w^{(r)}_{ij}|\n",
        "\\end{equation}\n",
        "\n",
        "At a high level L1 Regularization is similar to L2 Regularization since it leads to smaller weights (you will see the analogy in the next subsection). It results in the following weight update equation when using Stochastic Gradient Descent (where  sgn  is the sign function, such that  sgn(w)=+1  if  w>0 ,  sgn(w)=âˆ’1  if  $w<0$ , and sgn(0)=0 ):\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}â†w^{(r)}_{ij}âˆ’Î·Î»sgn(w^{(r)}_{ij})âˆ’Î·\\frac{\\partial L}{\\partial w_{ij}^{r}} \n",
        "\\end{equation}\n",
        "\n",
        "In the code which follows we will create an unregularised model, a L1 model, L2 model, and an elastic model and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHr7hyns-9K"
      },
      "source": [
        "##Unregularized Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aIo8e8NQfWQ"
      },
      "source": [
        "# Dataloaders for Regularization\n",
        "\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "#Splitting dataset\n",
        "reg_train_data, reg_val_data,_ = torch.utils.data.random_split(img_dataset, [30,100,14500])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "reg_train_loader = torch.utils.data.DataLoader(reg_train_data,batch_size=batch_size,worker_init_fn=seed_worker)\n",
        "reg_val_loader = torch.utils.data.DataLoader(reg_val_data,batch_size=1000,worker_init_fn=seed_worker)\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltJEADW-ou6H"
      },
      "source": [
        "Now let's train a model without any regularization and keep it aside as our bencmark for this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MM2k5L7QPrO4"
      },
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_unreg, train_acc_unreg,param_norm_unreg,_ ,_ = main(args, model, reg_train_loader, reg_val_loader, img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_unreg,label='Val Accuracy',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_unreg,label='Train Accuracy',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_unreg),c = 'green',ls = 'dashed')\n",
        "plt.title('Unregularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_unreg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFrTpy6juYIF"
      },
      "source": [
        "def l1_reg(model):\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch model \n",
        "    This function calculates the l1 norm of the all the tensors in the model\n",
        "  \"\"\"\n",
        "  l1 = 0\n",
        "\n",
        "  for param in model.parameters():\n",
        "    l1 += torch.sum(torch.abs(param))\n",
        "\n",
        "  return l1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AdcEcthXlQA"
      },
      "source": [
        "\n",
        "net = nn.Linear(20,20)\n",
        "print(f'L1 norm of the model: {l1_reg(net)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BUOKB0D7oC4"
      },
      "source": [
        "# here is an example of model setting. But if you don't want to use their dataset, you may want to rewrite this part.\n",
        "class Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(104)\n",
        "        super(Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 128)\n",
        "        self.fc2 = nn.Linear(128, 32)\n",
        "        self.fc3 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt-kt7AqHe7j"
      },
      "source": [
        "## Lambda Parameter for Regularisation\n",
        "\n",
        "You can see below we have a new argument in the argument dict - the lambda value. This is the same lambda we saw in the L1 expression above, and we will see it again with the other regularisation methods. Lamda values often range from 0-5. We start with a value of 0.1, and the ideal way to identify the lambda value is to do a grid optimisation (i.e run it for various values and see what works best). 1 is considered a large lambda value, and often we search between [0.1, 0.2 ... 0.9, 1, 2, 5].\n",
        "\n",
        "Here is a useful Stackoverflow link with information related to the topic - [calculating lambda value](https://stackoverflow.com/questions/12182063/how-to-calculate-the-regularization-parameter-in-linear-regression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "815wz2icujZa"
      },
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda': 0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l1reg, train_acc_l1reg,param_norm_l1reg,_,_ = main(args, model, reg_train_loader, reg_val_loader, img_test_dataset, reg_function1=l1_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l1reg,label='Val Accuracy L1 Regularized',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l1reg,label='Train Accuracy L1 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L1 regularized model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l1reg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HuDsreduuem"
      },
      "source": [
        "##L2 / Ridge Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik7S1rjp3cIz"
      },
      "source": [
        "L2 Regularization is a commonly used technique in ML systems is also sometimes referred to as â€œWeight Decayâ€. It works by adding a quadratic term to the Cross Entropy Loss Function  L , called the Regularization Term, which results in a new Loss Function  LR  given by:\n",
        "\n",
        "\\begin{equation}\n",
        "LR=L+Î»âˆ‘(w^{(r)}_{ij})^2\n",
        "\\end{equation}\n",
        "\n",
        "In order to get further insight into L2 Regularization, we investigate its effect on the Gradient Descent based update equations for the weight and bias parameters. Taking the derivative on both sides of the above equation, we obtain\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial L_r}{\\partial w^{(r)}_{ij}}=\\frac{\\partial L}{\\partial w^{(r)}_{ij}}+Î»w^{(r)}_{ij}\n",
        "\\end{equation}\n",
        "Thus the weight update rule becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}â†w^{(r)}_{ij}âˆ’Î·\\frac{\\partial L}{\\partial W^{(r)}_{ij}}âˆ’Î·Î»w^{(r)}_{ij}=(1âˆ’Î·Î»)w^{(r)}_{ij}âˆ’Î·\\frac{\\partial L}{\\partial w^{(r)}_{ij}}\n",
        "\\end{equation}\n",
        "\n",
        "where, $\\eta$ is learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-4wP0puoPL"
      },
      "source": [
        "def l2_reg(model):\n",
        "\n",
        "  \"\"\"\n",
        "    Inputs: Pytorch model \n",
        "    This function calculates the l2 norm of the all the tensors in the model\n",
        "  \"\"\"\n",
        "\n",
        "  l2 = 0.0\n",
        "\n",
        "  for param in model.parameters():\n",
        "    l2 += torch.sum(torch.abs(param)**2)\n",
        "  \n",
        "  return l2\n",
        "\n",
        "net = nn.Linear(20,20)\n",
        "print(f'L2 norm of the model: {l2_reg(net)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2hYvxMTxarr"
      },
      "source": [
        "here they just run a L1 norm model and L2 norm model and see whether the accuracy increases (and visualize them)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOZrOIGlv8tv"
      },
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda': 0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l2reg, train_acc_l2reg,param_norm_l2reg,model ,_ = main(args,model,train_loader,val_loader,img_test_dataset,reg_function1=l2_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l2reg,label='Val Accuracy L2 regularized',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l2reg,label='Train Accuracy L2 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l2reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L2 Regularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l2reg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFz0QB0hd387"
      },
      "source": [
        "##L1+L2 / Elastic net regularization\n",
        "\n",
        "Elastic Net regularization uses both L1 and L2 weights for regression. The loss function becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "LR=L+ Î»_{1}âˆ‘|w^{(r)}_{ij}| + Î»_{2}âˆ‘(w^{(r)}_{ij})^2\n",
        "\\end{equation}\n",
        "\n",
        "The weights update equation then becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "w^{(r)}_{ij}â†(1âˆ’Î·Î»_{2})w^{(r)}_{ij}âˆ’Î·Î»_{1}sgn(w^{(r)}_{ij})âˆ’Î·\\frac{\\partial L}{\\partial w_{ij}^{r}} \n",
        "\\end{equation}\n",
        "\n",
        "where, $\\eta$ is learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1b-FPIjxlQN"
      },
      "source": [
        "args = {'epochs': 150,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        'lambda1':0.1,\n",
        "        'lambda2':0.1\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_l1l2reg, train_acc_l1l2reg,param_norm_l1l2reg,model ,_ = main(args,model,train_loader,val_loader,img_test_dataset,reg_function1=l1_reg,reg_function2=l2_reg)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_l1l2reg,label='Val L1+L2',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l1l2reg,label='Train L1+L2',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1l2reg),c = 'green',ls = 'dashed')\n",
        "plt.title('L1+L2 Regularized Model')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "print('maximum Validation Accuracy reached:%f'%max(val_acc_l1l2reg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZ5KKRPiEXxQ"
      },
      "source": [
        "plt.plot(val_acc_l2reg,c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_l2reg,label='L2 regularized',c='red',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l2reg),c = 'red',ls = 'dashed')\n",
        "plt.plot(val_acc_l1reg,c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_l1reg,label='L1 regularized',c='green',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1reg),c = 'green',ls = 'dashed')\n",
        "plt.plot(val_acc_unreg,c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_unreg,label='Unregularized',c='blue',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_unreg),c = 'blue',ls = 'dashed')\n",
        "plt.plot(val_acc_l1l2reg,c='orange',ls = 'dashed')\n",
        "plt.plot(train_acc_l1l2reg,label='L1+L2 regularized',c='orange',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_l1l2reg),c = 'orange',ls = 'dashed')\n",
        "\n",
        "plt.title('Unregularized Vs L1-Regularized vs L2-regularized Vs L1+L2 regularized')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy(%)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdSHj2USHVqv"
      },
      "source": [
        "### Alternative way to implement Regularisation in PyTorch\n",
        "\n",
        "It is also possible to set up L2 regularisation (weight decay) by just using the ```weight_decay``` parameter in your optimisation method. Link to [optimisers documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam), and [discussion on L1 and L2 implementations on stackoverflow](https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch).\n",
        "\n",
        "### Regularisation with Keras\n",
        "\n",
        "[Keras documentation for layer regularizers](https://keras.io/api/layers/regularizers/)\n",
        "\n",
        "[How to use weight decay (L2 reg) with Keras](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FifeU3xZ2h0k"
      },
      "source": [
        "## Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBK9lE8P7WOd"
      },
      "source": [
        "### Dropout Implementation Caveats: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwVYGYd57lVm"
      },
      "source": [
        "\n",
        "*  Dropout is used only during training, during testing the complete model weights are used and hence it is important to use model.eval() before testing the model. \n",
        "\n",
        "* Dropout reduces the capacity of the model during training and hence as a general practice wider networks are used when using dropout. If you are using a dropout with a random probability of 0.5 then you might want to double the number of hidden neurons in that layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1WEkOApU_oo"
      },
      "source": [
        "Now, lets see how Dropout fares on the Animal Faces Dataset. We first modify the existing model to include dropouts and then train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46FM-qNM4q3-"
      },
      "source": [
        "##Network Class - Animal Faces\n",
        "class Animal_Net_Dropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(32)\n",
        "        super(Animal_Net_Dropout, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 248)\n",
        "        self.fc2 = nn.Linear(248, 210)\n",
        "        self.fc3 = nn.Linear(210, 3)\n",
        "        self.dropout1 = nn.Dropout(p = 0.5)\n",
        "        self.dropout2 = nn.Dropout(p = 0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.dropout1(self.fc1(x)))\n",
        "        x =F.leaky_relu(self.dropout2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qYd11zF4mr_"
      },
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'batch_size': 32,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'seed': 1,\n",
        "        'log_interval': 100\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net_Dropout()\n",
        "\n",
        "val_acc_dropout, train_acc_dropout, _, model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_dropout,label='Val - DP',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_dropout,label='Train - DP',c='red',ls = 'solid')\n",
        "plt.title('Dropout')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYf2EW344KeD"
      },
      "source": [
        "When do you think dropouts can perform bad and do you think their placement within a model matters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb-vmSymLVSE"
      },
      "source": [
        "### Dropout for Keras\n",
        "\n",
        "We saw in the last tutorial how we can use dropout for Keras - similar to PyTorch, it is just adding a layer in your model.\n",
        "\n",
        "[Keras Documentation](https://keras.io/api/layers/regularization_layers/dropout)\n",
        "\n",
        "[Blog post on Dropout with Keras](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2uz-1Yw85Qb"
      },
      "source": [
        "## Data Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIkE-h63IlZ"
      },
      "source": [
        "We will explore the effects of Data Augmentation on regularization. Here regularization is acheived by adding noise into training data after every epoch.\n",
        "\n",
        "Pytorch's torchvision module provides a few inbuilt data augmentation techniques which we can use on image datasets. Some of the techniques we most frequently use are:\n",
        "\n",
        "\n",
        "*   Random Crop\n",
        "*   Random Rotate\n",
        "*   Vertical Flip\n",
        "*   Horizontal Flip\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX7jk4FEbbU3"
      },
      "source": [
        "##Data Augmentation using transforms\n",
        "new_transforms = transforms.Compose([\n",
        "                                     transforms.RandomHorizontalFlip(p=0.1),\n",
        "                                     transforms.RandomVerticalFlip(p=0.1),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=new_transforms)\n",
        "#Splitting dataset\n",
        "new_train_data, _,_ = torch.utils.data.random_split(img_dataset, [250,100,14280])\n",
        "\n",
        "#Creating train_loader and Val_loader\n",
        "new_train_loader = torch.utils.data.DataLoader(new_train_data,batch_size=batch_size,worker_init_fn=seed_worker)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHm_wRhw1pqM"
      },
      "source": [
        "args = {'epochs': 250,\n",
        "        'lr': 1e-3,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cW8KjkpvnqS"
      },
      "source": [
        "\n",
        "acc_dict = {}\n",
        "model = Animal_Net()\n",
        "\n",
        "val_acc_dataaug, train_acc_dataaug, param_norm_datadug, _ ,_ = main(args,model,new_train_loader,val_loader,img_test_dataset)\n",
        "model = Animal_Net()\n",
        "val_acc_pure, train_acc_pure, param_norm_pure,_,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "\n",
        "##Train and Test accuracy plot\n",
        "\n",
        "plt.plot(val_acc_pure,label='Val Accuracy Pure',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_pure,label='Train Accuracy Pure',c='red',ls = 'solid')\n",
        "\n",
        "plt.plot(val_acc_dataaug,label='Val Accuracy data augment',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc_dataaug,label='Train Accuracy data augment',c='blue',ls = 'solid')\n",
        "plt.axhline(y=max(val_acc_pure),c = 'red',ls = 'dashed')\n",
        "plt.axhline(y=max(val_acc_dataaug),c = 'blue',ls = 'dashed')\n",
        "plt.title('Data Augmentation')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnjS44wuL493"
      },
      "source": [
        "plt.plot(param_norm_pure,c='red',label = 'Without Augmentation')\n",
        "plt.plot(param_norm_datadug,c='blue',label='With Augmentation')\n",
        "plt.title('Norm of parameters as a function of training epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Norm of model parameters')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTZg4iS-bMGI"
      },
      "source": [
        "### Data Augmentation for Keras\n",
        "\n",
        "MixUp augmentation for image classification - https://keras.io/examples/vision/mixup/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OySRfKnQGbhW"
      },
      "source": [
        "## Batch Size\n",
        "Batch size, in some cases, can also help in regularizing the models. Lower batch size leads to a noisy convergence and hence helps in converging to a broader local minima. Whereas, higher batch size lead to a smoother convergence thereby converging easily to a  deeper local minima.  This can be good or bad.\n",
        "\n",
        "In the below blcok we will train the Animal Net model with different batch sizes and see how that is going to affect the regularization performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "eBKF6O4KMUpr"
      },
      "source": [
        "#@title Dataset for Batch_size\n",
        "data_path = pathlib.Path('.')/'afhq' # using pathlib to be compatible with all OS's\n",
        "img_dataset = ImageFolder(data_path/'train', transform=train_transform)\n",
        "\n",
        "#Splitting dataset\n",
        "reg_train_data, reg_val_data,_ = torch.utils.data.random_split(img_dataset, [250,100,14280])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQbO03j8GjUX"
      },
      "source": [
        "args = {'lr': 5e-3,\n",
        "        'epochs': 60,\n",
        "        'momentum': 0.99,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "batch_sizes = [32,64,128]\n",
        "acc_dict = {}\n",
        "\n",
        "for i in range(len(batch_sizes)):\n",
        "    model = Animal_Net()\n",
        "    #Creating train_loader and Val_loader\n",
        "    reg_train_loader = torch.utils.data.DataLoader(reg_train_data,batch_size=batch_sizes[i],worker_init_fn=seed_worker)\n",
        "    reg_val_loader = torch.utils.data.DataLoader(reg_val_data,batch_size=1000,worker_init_fn=seed_worker)\n",
        "    val_acc, train_acc,param_norm,_,_ = main(args,model,reg_train_loader,reg_val_loader,img_test_dataset)\n",
        "    acc_dict['train_'+str(i)] = train_acc\n",
        "    acc_dict['val_'+str(i)] = val_acc\n",
        "    acc_dict['param_norm'+str(i)] = param_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifx1rCTiG-tW"
      },
      "source": [
        "#Plot Train and Val curves\n",
        "plt.plot(acc_dict['train_0'], label='mb_size =' + str(batch_sizes[0]), c = 'blue')\n",
        "plt.plot(acc_dict['val_0'], linestyle='dashed', c = 'blue')\n",
        "\n",
        "plt.plot(acc_dict['train_1'], label='mb_size =' + str(batch_sizes[1]), c = 'orange')\n",
        "plt.plot(acc_dict['val_1'], linestyle='dashed', c = 'orange')\n",
        "plt.plot(acc_dict['train_2'], label='mb_size =' + str(batch_sizes[2]), c = 'green')\n",
        "plt.plot(acc_dict['val_2'], linestyle='dashed', c = 'green')\n",
        "print('maximum accuracy for mini batchsize = 32: '+str(max(acc_dict['val_0'])))\n",
        "print('maximum accuracy for mini batchsize = 64: '+str(max(acc_dict['val_1'])))\n",
        "print('maximum accuracy for mini batchsize = 128: '+str(max(acc_dict['val_2'])))\n",
        "\n",
        "plt.title('Optimal Batch Size')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch (Sec)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Kp_jf37nnS"
      },
      "source": [
        "# Plot Parametric Norms\n",
        "plt.plot(acc_dict['param_norm0'],c='blue',label='mb_size =' + str(batch_sizes[0]))\n",
        "plt.plot(acc_dict['param_norm1'],c='orange',label='mb_size =' + str(batch_sizes[1]))\n",
        "plt.plot(acc_dict['param_norm2'],c='green',label='mb_size =' + str(batch_sizes[2]))\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Parameter Norm')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_Sbvul0O9AP"
      },
      "source": [
        "Here what observation can you make for different batch size. Why do you think this is happening?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5MCqOKVOF4x"
      },
      "source": [
        "## Pruning\n",
        "\n",
        "Google is known for training very big language models and recently it trained a [trillion paramter](https://thenextweb.com/neural/2021/01/13/googles-new-trillion-parameter-ai-language-model-is-almost-6-times-bigger-than-gpt-3/) model. This is almost 1.2e6 times bigger than the models we have been training. So it is sufficient to say that these big models need intense compute power to train while also becomeing harder to deploy and get real time inference on smaller micro - proccesors. \n",
        "\n",
        "This is where regularization and pruning come in very handy. Until now you should have noticed that the Frobenious norm of the regularized models that we trained tend to be smaller than those of unregualrized models. This indicates that the regualarization is shrinking the weights (making the model sparser) while improving the test performance. \n",
        "\n",
        "While methods like L1 regulartization promote implicit sparsity, in pruning we explicitly set a few weights of the trained model to zero and then retrain the model to adjust the other weights. This reduces the memory consumption, improves inference and helps the planet :)\n",
        "\n",
        "One of the most common methods of pruning a NN is to zero out a certain percentage of parameters based on their L1 norm. We don't actually remove the parameters because that makes forward computations difficult.\n",
        "\n",
        "Luckily we have Pytorch's torch.nn.utils.prune methods to play around and test pruning.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuXT6_dGOffv"
      },
      "source": [
        "def prune_l1_unstructured(model,prune_percent_weight,prune_percent_bias = 0):\n",
        "\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=prune_percent_weight)\n",
        "            prune.l1_unstructured(module, name='bias', amount=prune_percent_bias)\n",
        "\n",
        "            print(\n",
        "                \"Sparsity in {}: {:.2f}%\".format(name,\n",
        "                    100. * float(torch.sum(module.weight == 0))\n",
        "                    / float(module.weight.nelement())\n",
        "                )\n",
        "            )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zliK5QbzOj4B"
      },
      "source": [
        "##uncomment to run the test\n",
        "test_model = Animal_Net()\n",
        "prune_percent = 0.15\n",
        "prune_l1_unstructured(test_model,0.15)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkLl7xqhOnT_"
      },
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 5e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "model = Big_Animal_Net()\n",
        "prune_percent = 0.5\n",
        "\n",
        "print(\"Training a randomly initialized model\")\n",
        "val_acc, train_acc, _, trained_model ,_ = main(args,model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "##pruning a model\n",
        "print('Pruning and verifying and model:')\n",
        "prune_l1_unstructured(trained_model,prune_percent)\n",
        "\n",
        "#training the pruned model\n",
        "print(\"Training a pruned model\")\n",
        "val_acc_prune, train_acc_prune, _, pruned_model ,_ = main(args,trained_model.to('cpu'),train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "val_acc_prune = [val_acc_prune[0]]*args['epochs'] + val_acc_prune\n",
        "train_acc_prune = [train_acc_prune[0]]*args['epochs'] + train_acc_prune\n",
        "plt.plot(val_acc,label='Val',c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc,label='Train',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_prune,label='Val Prune',c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_prune,label='Train Prune',c='red',ls = 'solid')\n",
        "plt.title('Pruning')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUVBxu7qOq0K"
      },
      "source": [
        "Now change the prune_percent and report the percentage at which the model underfits. Let us say you create a new model with number of parameters equal to the number of parameters left after pruning. Do you think this model will work as good as the model which get after pruning the larger network? In the above pruning technique after pruning the network, how do you think the performance of the will change if we re-initialize the weights while maintaing the prune mask?\n",
        "\n",
        "### Pruning in Keras\n",
        "\n",
        "https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8ILhtBu6Ivq"
      },
      "source": [
        "## Lottery Tickets\n",
        "\n",
        "\n",
        "The lottery ticket hypothesis claims that \" A dense randomly initialized NN contains a subnetwork that is initialzed such that when trained in isolation it can match the test accuracy of the original network after training for at most same number of iterations\" i.e. a pruned model when reinitialized with the same weights will can match the test accuracy of the denser model. If the initialization changes the accuracy match is no longer guaranteed.\n",
        "\n",
        "Here we train the following networks:\n",
        "\n",
        "An unregularized model with Xavier initialization of weights for 200 epochs\n",
        "A Pruned model with the weights reinitialized to the random values.\n",
        "A pruned model with weights initialized with same Xavier Initialization as unregularized model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPLz-Jj2O7cz"
      },
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'lr': 1e-3,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        }\n",
        "\n",
        "acc_dict = {}\n",
        "init_model = Big_Animal_Net()\n",
        "xavier_model = Big_Animal_Net()\n",
        "prune_percent = 0.4\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD0AqEjoO-Gj"
      },
      "source": [
        "#Xavier Initilaization for one of the two models\n",
        "for name, module in xavier_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(module.weight)\n",
        "\n",
        "print('Training the full model')\n",
        "val_acc, train_acc, _, trained_model ,_ = main(args,copy.deepcopy(xavier_model),train_loader,val_loader,img_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGRy_PGzPCm6"
      },
      "source": [
        "#prune the trained model\n",
        "prune_l1_unstructured(trained_model,prune_percent)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVi9Q5PHVi"
      },
      "source": [
        "#initialize masks for the initialzed model and xavier model\n",
        "for name, module in init_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        prune.identity(module, name='weight')\n",
        "        prune.identity(module, name='bias')\n",
        "\n",
        "for name, module in xavier_model.named_modules():\n",
        "    if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "        prune.identity(module, name='weight')\n",
        "        prune.identity(module, name='bias')\n",
        "\n",
        "init_modules = [[name,module] for name, module in init_model.named_modules()]\n",
        "xavier_modueles = [[name,module] for name, module in xavier_model.named_modules()]\n",
        "trained_modules = [[name,module] for name, module in trained_model.named_modules()]\n",
        "\n",
        "for i in range(len(init_modules)):\n",
        "    if isinstance(init_modules[i][1], torch.nn.Conv2d) or isinstance(init_modules[i][1], torch.nn.Linear):\n",
        "        init_modules[i][1].weight_mask = copy.deepcopy(trained_modules[i][1].weight_mask) \n",
        "        init_modules[i][1].bias_mask = copy.deepcopy(trained_modules[i][1].bias_mask)\n",
        "\n",
        "for i in range(len(xavier_modueles)):\n",
        "    if isinstance(xavier_modueles[i][1], torch.nn.Conv2d) or isinstance(xavier_modueles[i][1], torch.nn.Linear):\n",
        "        xavier_modueles[i][1].weight_mask = copy.deepcopy(trained_modules[i][1].weight_mask) \n",
        "        xavier_modueles[i][1].bias_mask = copy.deepcopy(trained_modules[i][1].bias_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgOi5hoO6UE1"
      },
      "source": [
        "print('Training the pruned and Xavier model')\n",
        "val_acc_lottery_x, train_acc_lottery_x, _, pruned_model_x ,_ = main(args,xavier_model,train_loader,val_loader,img_test_dataset)\n",
        "print('Training the pruned Init model')\n",
        "val_acc_lottery, train_acc_lottery, _, pruned_model ,_ = main(args,init_model,train_loader,val_loader,img_test_dataset)\n",
        "\n",
        "plt.plot(val_acc,c='blue',ls = 'dashed')\n",
        "plt.plot(train_acc,label='Train - full model',c='blue',ls = 'solid')\n",
        "plt.plot(val_acc_lottery,c='red',ls = 'dashed')\n",
        "plt.plot(train_acc_lottery,label='Train - Random',c='red',ls = 'solid')\n",
        "plt.plot(val_acc_lottery_x,c='green',ls = 'dashed')\n",
        "plt.plot(train_acc_lottery_x,label='Train - Xavier',c='green',ls = 'solid')\n",
        "plt.title('Lottery Tickets')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Cw5w-DBPLbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvKe0LT5RePu"
      },
      "source": [
        "### Lottery Ticket in Keras\n",
        "\n",
        "Keras Option: https://github.com/google-research/lottery-ticket-hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUtwddoZ9J4P"
      },
      "source": [
        "## Distillation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JPlxMQOpVmE"
      },
      "source": [
        "Bigger neural nets are better for model performance but require significant memory, while smaller networks tend be be less accurate but are easier to deploy and use. \n",
        "\n",
        "Distillation is a technique which allows us to train smaller networks such that they mimic the outputs of the bigger network. The bigger network is called the teacher network wheras the smaller one is the student network. \n",
        "\n",
        "Distillation begins by training a teacher network. It then trains the student network with both the original labels and \"soft\" labels--the output of the teacher model. This lets us train the student network on unlabelled datasets, using the labeled given by the teacher network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX6oKO3oDS5e"
      },
      "source": [
        "Let's begin by desiging a smaller network and training the parent model and also the small model using hard labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHA10EuJrmjV"
      },
      "source": [
        "class Small_Animal_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        torch.manual_seed(32)\n",
        "        super(Small_Animal_Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(3*32*32, 32)\n",
        "        self.fc2 = nn.Linear(32, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.shape[0],-1)\n",
        "        x = F.leaky_relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-zsV74w3P8m"
      },
      "source": [
        "#Train the models \n",
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'lr' : 5e-3,\n",
        "        'cross_entropy':True\n",
        "        }\n",
        "\n",
        "Bmodel = Big_Animal_Net()\n",
        "Smodel = Small_Animal_Net()\n",
        "\n",
        "val_acc_big, train_acc_big, _, trained_big_model ,_ = main(args,Bmodel,train_loader,val_loader,img_test_dataset)\n",
        "val_acc_small, train_acc_small, _, _ ,_ = main(args,Smodel,train_loader,val_loader,img_test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b8BnQzOrb7t"
      },
      "source": [
        "Loss Function:\n",
        "\n",
        "We use the same cross entropy loss as before but we use both hard and soft labels to calculate loss. We cannot directly use the CrossEntropy Loss in Pytorch to calculate loss for soft labels. Hence we will exploit the relation between Cross Entropy and KL Divergence. \n",
        "\n",
        "Cross Entropy loss and KL Divergence are both related by: H(p,q) = H(p) + KL(p,q) where H(p,q) calculates cross entropy loss between distributions p and q wheras KL represents KL divergence. Here p is the probability distribution of soft_outputs which are constant and hence we can omit from the loss function.\n",
        "\n",
        "        L = (1 - alpha)*CE(outputs,ground_truth) + alpha * (T**2) *CE(outputs,soft_targets)\n",
        "        L = (1 - alpha)*CE(outputs,ground_truth) + alpha * (T**2) *KL(outputs,soft_targets)\n",
        "\n",
        "Here alpha and temperature are hyper parameters where temperatures is used to smoothen the outputs of the parent network. \n",
        "\n",
        "[Click to learn more about the relation](https://adventuresinmachinelearning.com/cross-entropy-kl-divergence/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKhrkVwY6Wlq"
      },
      "source": [
        "def distillation_loss(args,soft_outputs, pred_logits, target):\n",
        "\n",
        "    alpha = args['alpha']\n",
        "    T = args['temperature']\n",
        "    dist_loss = (1. - alpha) * F.cross_entropy(pred_logits, target) + \\\n",
        "                    (alpha * (T ** 2)) * F.kl_div(F.log_softmax(pred_logits/T, dim=1),\n",
        "                             F.softmax(soft_outputs/T, dim=1))\n",
        "\n",
        "    return dist_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH5qZx-HQcDT"
      },
      "source": [
        "#Modified Train Functions \n",
        "def train_softmax_distillation(args, student_model,parent_model, device, train_loader, optimizer, epoch):\n",
        "    \n",
        "    student_model.train()\n",
        "    parent_model.eval()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        soft_outputs = parent_model(data)\n",
        "        pred_logits = student_model(data)\n",
        "        loss = distillation_loss(args,soft_outputs.detach(), pred_logits, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXaMJOfVP0TL"
      },
      "source": [
        "#Modified Main Function\n",
        "def distilation_main(args, teacher_model,student_model,train_loader,val_loader):\n",
        "\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "\n",
        "    student_model = student_model.to(device)\n",
        "    teacher_model = teacher_model.to(device)\n",
        "    optimizer = optim.SGD(student_model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "    val_acc_list = []\n",
        "    train_acc_list = []\n",
        "    for epoch in tqdm(range(1, args['epochs'] + 1)):\n",
        "        train_softmax_distillation(args, student_model,teacher_model, device, train_loader, optimizer, epoch)\n",
        "        train_acc = test(student_model,device,train_loader,'Train')\n",
        "        val_acc = test(student_model,device,val_loader,'Val')\n",
        "        val_acc_list.append(val_acc)\n",
        "        train_acc_list.append(train_acc)\n",
        "\n",
        "    return val_acc_list, train_acc_list, student_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o13VleaKFAtE"
      },
      "source": [
        "Now that we have everything ready let's train the student network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp-5pGaeubLc"
      },
      "source": [
        "args = {'test_batch_size': 1000,\n",
        "        'epochs': 200,\n",
        "        'momentum': 0.9,\n",
        "        'no_cuda': False,\n",
        "        'lr' : 5e-3,\n",
        "        'alpha': 1,\n",
        "        'temperature': 40\n",
        "        }\n",
        "\n",
        "student_model = Small_Animal_Net()\n",
        "\n",
        "val_acc_st, train_acc_st, _, = distilation_main(args,trained_big_model,student_model,train_loader,val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtRFEj_Uwq1l"
      },
      "source": [
        "# Plot the validation curves of student and the original small model \n",
        "plt.plot(val_acc_small,label='Val - Small Model',c='red',ls = 'dashed')\n",
        "plt.axhline(y = max(val_acc_small),c = 'red')\n",
        "plt.plot(val_acc_st,label='Val - Student Model',c='green',ls = 'dashed')\n",
        "plt.axhline(y = max(val_acc_st),c = 'green')\n",
        "plt.title('Distillation')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXhCP6P5Snd_"
      },
      "source": [
        "This method not only provides a way to train small and better networks but also gives us a chance to train small networks on more data using the soft labels from the heavily optimized parent networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUu5orrg0UHc"
      },
      "source": [
        "What other techniques can you use to reduce the dimensionality or size of the big network or the input data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SsdI6tVz96F"
      },
      "source": [
        "Do you think regularization helps when you have infinite data available?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJZMeMTm1NG6"
      },
      "source": [
        "Which regualarization technique from this week do you think had the biggest effect on the network and why do you think so? Can you apply all of them on the same network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExuS6qZMR__O"
      },
      "source": [
        "### Distillation in Keras\n",
        "\n",
        "https://keras.io/examples/vision/knowledge_distillation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoH-ohSBQduT"
      },
      "source": [
        "# Homework Notebooks\n",
        "\n",
        "In the last notebook, you trained your own Keras or PyTorch models on data of your choice. In this notebook, you are going to use those models (you can also train new models), and use different optimization and regularisation techniques on them. \n",
        "\n",
        "You will be graded for 4 out of 6 of the modeling methods you use.\n",
        "\n",
        "**1)** Build or reuse an older model you have built, and use 3 different optimization techniques or methods on the model. For one of these models, add a batch normalization layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOuwL7DRQfJZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51wu3oTEVk9I"
      },
      "source": [
        "**1a)** Which optimizer performed the best? How did Batch Normalization method effect your model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_9p2XLKV6Sj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaHT-vAbV6z0"
      },
      "source": [
        "**2)** Build or reuse an older model you have built, and use 3 different Regularization techniques or methods on the model. You will get bonus points if you use Pruning, Distillation, or the Lottery Ticket Hypothesis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acCw3oL6WK8j"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iSR_VtOWLrv"
      },
      "source": [
        "**2a)** Which regularisation technique worked best? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGAWOrAxWSEd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}